{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea662b73-b830-48d9-a4be-b8503c8f4293",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "3b0cf542-8b6a-4120-89b5-1a2feef51890",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 55bb623 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "p = pipeline(\"automatic-speech-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f14931b-0ac0-4b14-9979-ed06d2748fbc",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "5d86c52f-99d4-4dc7-aa0a-d7d720967118",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f97de6eb-dfd0-4c74-8e85-4c9cb69583af",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "8fa91d47-a1d5-440f-a88d-8131f6de3f70",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "class audi:\n",
    "    def __init__(self):\n",
    "        self.text_f = \" \"\n",
    "\n",
    "    def transcribe(self,audio, state=\"\"):\n",
    "        time.sleep(3)\n",
    "        text = p(audio)[\"text\"]\n",
    "        state += text + \" \"\n",
    "        self.text_f += text + \" \"\n",
    "        return state, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e27d9af-014a-4556-b445-7d40fb5e916d",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "b35482c8-5840-4e16-8bfe-d1fd5c71951c",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_2684\\21486067.py:6: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  gr.inputs.Audio(source=\"microphone\", type=\"filepath\"),\n",
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_2684\\21486067.py:6: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  gr.inputs.Audio(source=\"microphone\", type=\"filepath\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\gradio\\processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\gradio\\routes.py\", line 534, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\gradio\\route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\gradio\\blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\gradio\\blocks.py\", line 1185, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2106, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 833, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\gradio\\utils.py\", line 661, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_2684\\785993888.py\", line 7, in transcribe\n",
      "    text = p(audio)[\"text\"]\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 357, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1132, in __call__\n",
      "    return next(\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"C:\\Users\\vikas\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 482, in preprocess\n",
      "    raise ValueError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "ValueError: We expect a numpy ndarray as input, got `<class 'NoneType'>`\n"
     ]
    }
   ],
   "source": [
    "obj = audi()\n",
    "\n",
    "gr.Interface(\n",
    "    fn=obj.transcribe, \n",
    "    inputs=[\n",
    "        gr.inputs.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "        'state'\n",
    "    ],\n",
    "    outputs=[\n",
    "        \"textbox\",\n",
    "        \"state\"\n",
    "    ],\n",
    "    live=True).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b537a49f-ab56-4123-bc23-bb8835bf9530",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "3aa34e8c-ea4f-4315-ae9d-e2eba387a3cf",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  HALLO I AM IN ENGLAND '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.text_f"
   ]
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
