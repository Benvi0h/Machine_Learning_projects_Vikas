{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee722ea3-62a7-489f-9bdc-31ce7163f094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "Flash attention 2 is not installed\n",
      "playsound is relying on another python subprocess. Please use `pip install pygobject` if you want playsound to run more efficiently.\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import LLM_with_RAG\n",
    "from get_embedding_function import get_embedding_function\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import soundfile as sf\n",
    "from playsound import playsound\n",
    "\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "import wave\n",
    "import tempfile\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import argparse\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "# from get_embedding_function import get_embedding_function\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "# from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3f519-be00-47ac-8acc-fd998cefbd50",
   "metadata": {},
   "source": [
    "# Get the finetuned RAG LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0edf6386-f7ff-41fc-a7b2-13a6540cf3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = LLM_with_RAG.Rag_Llama(context_window=4096,\n",
    "#                 max_new_tokens=256,\n",
    "#                 generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "#                 system_prompt=\"\"\"\"\"\",\n",
    "#                 tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#                 model_name=\"finetuned_llama_medicare\",\n",
    "#                 device_map=\"cuda:0\",\n",
    "#                 model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd7fb26-3ffa-476c-a91f-96a68331fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = LLM_with_RAG.RagLlamaFAISS(context_window=4096,\n",
    "                max_new_tokens=256,\n",
    "                generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "                system_prompt=\"\"\"\"\"\",\n",
    "                tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                model_name=\"finetuned_llama_medicare\",\n",
    "                device_map=\"cuda:0\",\n",
    "                model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26245022-b3b1-4ed2-86e4-66f3186f0b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4459c6c1abc8414b87d10075f43d4892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 documents.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HuggingFaceEmbedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence-transformers/all-mpnet-base-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:58\u001b[39m, in \u001b[36mprepare\u001b[39m\u001b[34m(self, embedding_model, data_path)\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'HuggingFaceEmbedding' is not defined"
     ]
    }
   ],
   "source": [
    "obj.prepare(embedding_model=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    data_path=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f3b564-f694-4c80-af04-01c1301ef190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Querying: How did the Onam celebration go?\n",
      "\n",
      "The Onam celebration at the ancestral home in Alappuzha was a grand affair. The family gathered after many years, and the house was cleaned and decorated with intricate pookalams (floral rangolis). Everyone wore traditional attireâ€”men in mundus and women in kasavu sareesâ€”and the aroma of banana chips, avial, and payasam filled the house as Onasadya was prepared by the grandmothers and aunts. In the afternoon, they played traditional games like tug-of-war, uriyadi (pot breaking), and kaashum-paaram, with uncles cheering loudly and children tumbling with laughter. Even the most reserved elders were coaxed into joining the dance when someone started playing â€œThiruvathirakaliâ€ on a loudspeaker. As the sun set, everyone gathered for a family photoâ€”a rare moment where nearly four generations were present. Later, sitting together under the mango tree, the elders recalled stories of past Onams, comparing how times had changed yet the essence remained. It was a day that rekindled bonds\n"
     ]
    }
   ],
   "source": [
    "print(obj.call(\"How did the Onam celebration go?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c52a06-9ba4-4314-aaed-8ca7b48c8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = obj.call(\"How did the Onam celebration go?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388c3309-e553-44ed-86b7-c346eab89ec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query_engine = obj.call(\"How did the Onam celebration go?\",\n",
    "#                         embedding_model = \"sentence-transformers/all-mpnet-base-v2\", \n",
    "#                         data_path = \"./data\", \n",
    "#                         first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce67345-f1f0-4b15-98e5-777093868cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a554753-fd2d-4b70-a2df-fbca6d258f6c",
   "metadata": {},
   "source": [
    "# Speech To Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2b728-914c-4883-9269-037c69c36950",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"small\"  # medium is better \n",
    "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacebe7f-c62c-4711-95e5-d1ef1e29eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path=\"parler_tts_out_1.wav\"\n",
    "beam_size = 5  # You can adjust the beam size as needed\n",
    "segments, info = model.transcribe(audio_path, beam_size=beam_size)\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9fa45-39a3-4eb6-b118-28964e3f0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "for segment in segments:\n",
    "    prompt = segment.text\n",
    "    # print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282aa07-26d9-4c82-9f1f-acb8a9747bf0",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63d9ec97-ae33-4dcc-975d-6994d87e05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Analysis:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_sentiment_model(self, task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None):\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "        # self.model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "        self.classifier = pipeline(task=task, model=model, top_k=top_k)\n",
    "        return self.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a19f6fdb-699a-4875-b7f1-95ea3dddd916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "sentiment_obj = Sentiment_Analysis()\n",
    "sentiment_model = sentiment_obj.load_sentiment_model(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a67ac89e-7aec-44ec-97bc-c04adeb84d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_sentiment(prompt, sentiment_model):\n",
    "        sentences = [prompt]\n",
    "        \n",
    "        model_outputs = sentiment_model(sentences)\n",
    "        return model_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee7e1e71-bd7c-4555-84bf-efdefe62dc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'curiosity', 'score': 0.7736185789108276},\n",
       " {'label': 'neutral', 'score': 0.20892566442489624},\n",
       " {'label': 'confusion', 'score': 0.13161107897758484},\n",
       " {'label': 'surprise', 'score': 0.009433308616280556},\n",
       " {'label': 'annoyance', 'score': 0.007904467172920704},\n",
       " {'label': 'approval', 'score': 0.005477572791278362},\n",
       " {'label': 'disapproval', 'score': 0.004325741436332464},\n",
       " {'label': 'excitement', 'score': 0.004149579908698797},\n",
       " {'label': 'disappointment', 'score': 0.004062008578330278},\n",
       " {'label': 'admiration', 'score': 0.0038862167857587337},\n",
       " {'label': 'realization', 'score': 0.003728482872247696},\n",
       " {'label': 'optimism', 'score': 0.0026969765312969685},\n",
       " {'label': 'love', 'score': 0.002558246487751603},\n",
       " {'label': 'anger', 'score': 0.0021130945533514023},\n",
       " {'label': 'sadness', 'score': 0.0019920726772397757},\n",
       " {'label': 'desire', 'score': 0.0019843177869915962},\n",
       " {'label': 'caring', 'score': 0.0018613188294693828},\n",
       " {'label': 'joy', 'score': 0.0018437252147123218},\n",
       " {'label': 'amusement', 'score': 0.0017007506685331464},\n",
       " {'label': 'fear', 'score': 0.0013616690412163734},\n",
       " {'label': 'disgust', 'score': 0.0010265380842611194},\n",
       " {'label': 'gratitude', 'score': 0.0009361766860820353},\n",
       " {'label': 'nervousness', 'score': 0.0008363305823877454},\n",
       " {'label': 'embarrassment', 'score': 0.000761301489546895},\n",
       " {'label': 'remorse', 'score': 0.0005659139715135098},\n",
       " {'label': 'grief', 'score': 0.00039265837403945625},\n",
       " {'label': 'relief', 'score': 0.00021894487144891173},\n",
       " {'label': 'pride', 'score': 6.145906081655994e-05}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_list = call_sentiment(prompt, sentiment_model)\n",
    "sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b664a6b-2269-4243-bd77-d2ddba1b35cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The person is 0.774% feeling curiosity, answer based on this feeling in mind as this is a alzhimer's patient\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feeling_of_person = f\"The person is {round(sentiment_list[0]['score'], 3)}% feeling {sentiment_list[0]['label']}, answer based on this feeling in mind as this is a alzhimer's patient\"\n",
    "feeling_of_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3659dec0-30f6-47ff-b3d4-ddb4c2d691aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    'joy': 0.0,\n",
    "    'excitement': 0.0,\n",
    "    'optimism': 0.0,\n",
    "    'love': 0.0,\n",
    "    'amusement': 0.0,\n",
    "    'gratitude': 0.0,\n",
    "    'surprise': 0.0,\n",
    "    'relief': 0.0,\n",
    "    'pride': 0.0,\n",
    "    'neutral': 0.0,\n",
    "    'approval': 0.0,\n",
    "    'realization': 0.0,\n",
    "    'admiration': 0.0,\n",
    "    'caring': 0.0,\n",
    "    'curiosity': 0.0,\n",
    "    'embarrassment': 0.0,\n",
    "    'nervousness': 0.0,\n",
    "    'sadness': 0.0,\n",
    "    'disappointment': 0.0,\n",
    "    'confusion': 0.0,\n",
    "    'disapproval': 0.0,\n",
    "    'fear': 0.0,\n",
    "    'desire': 0.0,\n",
    "    'grief': 0.0,\n",
    "    'remorse': 0.0,\n",
    "    'annoyance': 0.0,\n",
    "    'anger': 0.0,\n",
    "    'disgust': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb7ed5a7-b259-4573-a1f6-4432ff38e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_ids = {\n",
    "    'joy': 0,\n",
    "    'excitement': 1,\n",
    "    'optimism': 2,\n",
    "    'love': 3,\n",
    "    'amusement': 4,\n",
    "    'gratitude': 5,\n",
    "    'surprise': 6,\n",
    "    'relief': 7,\n",
    "    'pride': 8,\n",
    "    'neutral': 9,\n",
    "    'approval': 10,\n",
    "    'realization': 11,\n",
    "    'admiration': 12,\n",
    "    'caring': 13,\n",
    "    'curiosity': 14,\n",
    "    'embarrassment': 15,\n",
    "    'nervousness': 16,\n",
    "    'sadness': 17,\n",
    "    'disappointment': 18,\n",
    "    'confusion': 19,\n",
    "    'disapproval': 20,\n",
    "    'fear': 21,\n",
    "    'desire': 22,\n",
    "    'grief': 23,\n",
    "    'remorse': 24,\n",
    "    'annoyance': 25,\n",
    "    'anger': 26,\n",
    "    'disgust': 27\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00cab3ee-533b-4d4e-83c5-821cdf3939b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ = 0.0\n",
    "\n",
    "for senti in sentiment_list:\n",
    "    emotions[senti['label']] = round(senti['score'], 4)\n",
    "\n",
    "    if emotions[senti['label']] > max_:\n",
    "        max_ = emotions[senti['label']]\n",
    "        max_emotion = emotion_ids[senti['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cef30d4-625d-4156-9142-6bd536904644",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_list = []\n",
    "for i in emotions:\n",
    "    normal_list.append(emotions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f98f7c6-3f03-4963-beeb-92eb2864bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8323ede-831c-459d-a676-85129b8700e0",
   "metadata": {},
   "source": [
    "# Query for response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eaa09009-13d8-44f2-b9d6-09d434bc3a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The person is 0.774% feeling curiosity, answer based on this feeling in mind as this is a alzhimer's patientThe Onam celebration at the ancestral home in Alappuzha was a grand affair with traditional games, food, and cultural performances. The family wore traditional attire, and the men played Onasadya, a traditional Onam dish. Even the most reserved elders were coaxed into joining the dance, and the women tied mango leaves and turmeric stalks around the kitchen. It was a day that rekindled bonds and reconnected everyone with their roots. \""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feeling_of_person + prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc0c9c71-db71-4f22-a3a3-08e0ab4f4e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Querying:  How did the Onam celebration go?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Onam celebration at the ancestral home in Alappuzha was a grand affair with traditional games, food, and cultural performances. The family wore traditional attire, and the men played Onasadya, a traditional Onam dish. Even the most reserved elders were coaxed into joining the dance, and the women tied mango leaves and turmeric stalks around the kitchen. It was a day that rekindled bonds and reconnected everyone with their roots. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = obj.call(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42036209-7529-4bcf-b5f6-2462d9041ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"small\"  # medium is better \n",
    "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "534c715b-9102-49c1-a1ec-83a8c09643f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'parler_tts.dac_wrapper.modeling_dac.DACModel'> is overwritten by shared audio_encoder config: DACConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khZ_8kbps\",\n",
      "  \"architectures\": [\n",
      "    \"DACModel\"\n",
      "  ],\n",
      "  \"codebook_size\": 1024,\n",
      "  \"frame_rate\": 86,\n",
      "  \"latent_dim\": 1024,\n",
      "  \"model_bitrate\": 8,\n",
      "  \"model_type\": \"dac_on_the_hub\",\n",
      "  \"num_codebooks\": 9,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/decoder_400M/\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": false,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler_tts_mini_v0.1\", low_cpu_mem_usage=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler_tts_mini_v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62a41c83-4f67-4877-9a4b-c50e37de2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = response\n",
    "description = \"A mature male voice with a slight British accent, speaking in a professional hospital setting.\"\n",
    "\n",
    "input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c22234e5-f3fd-4450-aff3-2555bd0d1ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m generation = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m audio_arr = generation.cpu().numpy().squeeze()\n\u001b[32m      3\u001b[39m sf.write(\u001b[33m\"\u001b[39m\u001b[33mparler_tts_out_2.wav\u001b[39m\u001b[33m\"\u001b[39m, audio_arr, model.config.sampling_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/parler_tts/modeling_parler_tts.py:3564\u001b[39m, in \u001b[36mParlerTTSForConditionalGeneration.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, synced_gpus, streamer, **kwargs)\u001b[39m\n\u001b[32m   3556\u001b[39m     delayed_input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   3557\u001b[39m         input_ids=delayed_input_ids,\n\u001b[32m   3558\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   3559\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3560\u001b[39m         **model_kwargs,\n\u001b[32m   3561\u001b[39m     )\n\u001b[32m   3563\u001b[39m     \u001b[38;5;66;03m# 10. run sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3564\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3566\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3569\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3570\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3571\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3572\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3574\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3575\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3576\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGot incompatible mode for generation, should be one of greedy or sampling. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3577\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEnsure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3578\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/transformers/generation/utils.py:3223\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3220\u001b[39m next_token_logits = next_token_logits.to(input_ids.device)\n\u001b[32m   3222\u001b[39m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3223\u001b[39m next_token_scores = \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3225\u001b[39m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[32m   3226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/transformers/generation/logits_process.py:104\u001b[39m, in \u001b[36mLogitsProcessorList.__call__\u001b[39m\u001b[34m(self, input_ids, scores, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m         scores = processor(input_ids, scores, **kwargs)\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         scores = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "audio_arr = generation.cpu().numpy().squeeze()\n",
    "sf.write(\"parler_tts_out_2.wav\", audio_arr, model.config.sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
