{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5fed2b-585f-420d-8e2e-1cefbb18b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from textwrap import dedent\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from peft import LoraConfig, PeftModel, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, AutoConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09434b86-eedc-4947-8edf-fbaa51050283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03fe4583-8c42-43a0-8ca6-8c632c72e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(os.getcwd(), \"Model_cache_directory\")\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7788a78f-97e1-4493-91b0-51ca86e03dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f90c156fae542cbaf791f9ad2166411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model, \n",
    "    quantization_config = bnb_config, \n",
    "    device_map = {\"\":0},\n",
    "    trust_remote_code = True,\n",
    "    cache_dir = cache_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd87998-5043-4fbb-b986-2b55ec16ebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS Token :  </s>\n",
      "EOS Token ID :  2\n",
      "\n",
      "\n",
      "Pad Token :  None\n",
      "Pad Token ID :  None\n",
      "\n",
      "\n",
      "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, cache_dir = cache_dir)\n",
    "print(\"EOS Token : \",tokenizer.eos_token)\n",
    "print(\"EOS Token ID : \",tokenizer.eos_token_id)\n",
    "print(\"\\n\")\n",
    "print(\"Pad Token : \",tokenizer.pad_token)\n",
    "print(\"Pad Token ID : \",tokenizer.pad_token_id)\n",
    "print(\"\\n\")\n",
    "tokenizer.padding_side = 'left'\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300b4259-e1d5-46c2-b4ad-d30880172fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(row: dict):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        {row[\"questions\"]}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":\"Use only the information to answer the question\",\n",
    "        },\n",
    "        {\"role\" : \"user\", \"content\":prompt},\n",
    "        {\"role\" : \"assistant\", \"content\" : row[\"answers\"]}\n",
    "        \n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b9a75a-7662-49d4-9cc5-b056b7a1d94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Arjun Menon and what role does he play ...</td>\n",
       "      <td>Arjun Menon is a Bengaluru-based entrepreneur,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Oreo’s story and how did he impact Arj...</td>\n",
       "      <td>Oreo is a rescued black-and-white dog who was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who is Pavithra and what is her relationship w...</td>\n",
       "      <td>Pavithra is Arjun’s distant cousin and a passi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Joseph Uncle’s significance in Arjun’s...</td>\n",
       "      <td>Joseph Uncle, the family driver for over two d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is Rekha and what memories does she share ...</td>\n",
       "      <td>Rekha is Arjun’s childhood neighbor and first ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0  Who is Arjun Menon and what role does he play ...   \n",
       "1  What is Oreo’s story and how did he impact Arj...   \n",
       "2  Who is Pavithra and what is her relationship w...   \n",
       "3  What is Joseph Uncle’s significance in Arjun’s...   \n",
       "4  Who is Rekha and what memories does she share ...   \n",
       "\n",
       "                                             answers  \n",
       "0  Arjun Menon is a Bengaluru-based entrepreneur,...  \n",
       "1  Oreo is a rescued black-and-white dog who was ...  \n",
       "2  Pavithra is Arjun’s distant cousin and a passi...  \n",
       "3  Joseph Uncle, the family driver for over two d...  \n",
       "4  Rekha is Arjun’s childhood neighbor and first ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Family_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eca19fa-b47f-4819-98c7-178a43a22edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df.apply(format_example, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e0c5586-6dbd-48d4-b079-fffeff7a526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(row: Dict) -> int:\n",
    "    return len(\n",
    "        tokenizer(\n",
    "            row[\"text\"],\n",
    "            add_special_tokens = True,\n",
    "            return_attention_mask=False\n",
    "        )['input_ids']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb285a3-c9e5-468b-8e5a-5b72e015be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_count'] = df.apply(count_tokens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93c78132-47ad-4bc8-9f65-7620c74f810a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>text</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Arjun Menon and what role does he play ...</td>\n",
       "      <td>Arjun Menon is a Bengaluru-based entrepreneur,...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nUse only the information to...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Oreo’s story and how did he impact Arj...</td>\n",
       "      <td>Oreo is a rescued black-and-white dog who was ...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nUse only the information to...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who is Pavithra and what is her relationship w...</td>\n",
       "      <td>Pavithra is Arjun’s distant cousin and a passi...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nUse only the information to...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Joseph Uncle’s significance in Arjun’s...</td>\n",
       "      <td>Joseph Uncle, the family driver for over two d...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nUse only the information to...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is Rekha and what memories does she share ...</td>\n",
       "      <td>Rekha is Arjun’s childhood neighbor and first ...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nUse only the information to...</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0  Who is Arjun Menon and what role does he play ...   \n",
       "1  What is Oreo’s story and how did he impact Arj...   \n",
       "2  Who is Pavithra and what is her relationship w...   \n",
       "3  What is Joseph Uncle’s significance in Arjun’s...   \n",
       "4  Who is Rekha and what memories does she share ...   \n",
       "\n",
       "                                             answers  \\\n",
       "0  Arjun Menon is a Bengaluru-based entrepreneur,...   \n",
       "1  Oreo is a rescued black-and-white dog who was ...   \n",
       "2  Pavithra is Arjun’s distant cousin and a passi...   \n",
       "3  Joseph Uncle, the family driver for over two d...   \n",
       "4  Rekha is Arjun’s childhood neighbor and first ...   \n",
       "\n",
       "                                                text  token_count  \n",
       "0  <s>[INST] <<SYS>>\\nUse only the information to...          149  \n",
       "1  <s>[INST] <<SYS>>\\nUse only the information to...          150  \n",
       "2  <s>[INST] <<SYS>>\\nUse only the information to...          122  \n",
       "3  <s>[INST] <<SYS>>\\nUse only the information to...          126  \n",
       "4  <s>[INST] <<SYS>>\\nUse only the information to...          127  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df124e9c-b79d-4a26-8312-22134ba2a8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "Use only the information to answer the question\n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "What is Oreo’s story and how did he impact Arjun’s life? [/INST] Oreo is a rescued black-and-white dog who was adopted by Arjun and his wife Malini. Initially reluctant, Arjun quickly formed a strong bond with Oreo, who brought joy, discipline, and emotional support during tough times, including business challenges. Oreo became a symbolic presence in Arjun’s life, embodying trust, playfulness, and unconditional love. </s>\n"
     ]
    }
   ],
   "source": [
    "print(df['text'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d76cbd-cdde-4790-b94b-4e5c83c0bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, temp = train_test_split(df, test_size = 0.2)\n",
    "val, test = train_test_split(temp, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7bdc4dc-f4bd-4db8-af7f-587a4d0bc971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 10, 38)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test), len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc2fb27b-24be-4cea-a0d2-b906a52deb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_json(\"train.json\", orient=\"records\", lines = True)\n",
    "val.to_json(\"val.json\", orient=\"records\", lines = True)\n",
    "test.to_json(\"test.json\", orient=\"records\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a870d7-cae6-44a2-b4f4-32fdbfed29e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53bce2c74864ed6aecdeb568fa5108c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b740a6d2f5c74e2d9d4294c0afe66dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db1683b20e043d1a5b6a029096f48a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"train\" : \"train.json\", \"validation\" : \"val.json\", \"test\" : \"test.json\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "972e3177-6486-4e30-8467-920b5bd7082e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['questions', 'answers', 'text', 'token_count'],\n",
       "        num_rows: 189\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['questions', 'answers', 'text', 'token_count'],\n",
       "        num_rows: 38\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['questions', 'answers', 'text', 'token_count'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4368ec39-8ef7-472a-bdd8-b21cb91102c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\nUse only the information to answer the question\\n<</SYS>>\\n\\n\\nIn what ways did Arjun’s experiences during family storytelling nights nurture his imagination and connection to his heritage? [/INST] These evenings filled with folk tales and family anecdotes enriched Arjun’s sense of identity and creative thinking. </s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09ee43-dd0d-4331-a5e9-d14227a53f9e",
   "metadata": {},
   "source": [
    "# Original model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a9b2e35-0a43-45ee-98c4-7e20319395ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens = 128,\n",
    "    return_full_text = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9f983f6-11a5-4203-b2a0-d3bbf10d55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_prompt(data_row):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        {row[\"questions\"]}\n",
    "        You are an assistant for someone who is a patient with dementia, give appropriate and kind responses.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":\"Use only the information to answer the question\",\n",
    "        },\n",
    "        {\"role\" : \"user\", \"content\":prompt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfb831a5-8760-488d-8b70-56e768b22763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "Use only the information to answer the question\n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "What lessons did Arjun learn from his uncle Sajan’s journey as a successful entrepreneur about balancing risk and reward?\n",
      "You are an assistant for someone who is a patient with dementia, give appropriate and kind responses. [/INST]\n"
     ]
    }
   ],
   "source": [
    "row = dataset[\"test\"][0]\n",
    "prompt = create_test_prompt(row)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "827a76dd-8805-4ec3-baa0-42e32c114453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    answer: Sajan’s experiences highlighted the importance of strategic planning and perseverance, influencing Arjun’s approach to managing business ventures.\n",
      "    prediction:   Ah, an excellent question! Arjun's uncle Sajan's journey as a successful entrepreneur taught him many valuable lessons about balancing risk and reward. Here are some of the key lessons he learned:\n",
      "\n",
      "1. Understanding risk: Arjun realized that taking calculated risks is essential for success in business. His uncle Sajan showed him that it's important to assess the potential risks and rewards of any venture and make informed decisions.\n",
      "2. Embracing failure: Sajan shared with Arjun that he learned to embrace failure as\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(prompt)\n",
    "response = f\"\"\"\n",
    "    answer: {row[\"answers\"]}\n",
    "    prediction: {outputs[0][\"generated_text\"]}\n",
    "\"\"\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c32eb0c5-1482-4f5b-9024-3cdd8ccd4329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "Use only the information to answer the question\n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "Who is Malini and what is her relationship with Arjun?\n",
      "You are an assistant for someone who is a patient with dementia, give appropriate and kind responses. [/INST]\n"
     ]
    }
   ],
   "source": [
    "row = dataset[\"test\"][1]\n",
    "prompt = create_test_prompt(row)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d9d6cee-712a-4aa7-b9be-b169c06d4092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    answer: Malini is Arjun’s wife and childhood sweetheart. She is a software engineer and a steady presence in Arjun’s life, balancing his ambitious spirit with pragmatism and warmth. Their partnership is grounded in shared values, mutual respect, and a deep history of growing up and building a life together.\n",
      "    prediction:   Hello there! *smiling warmly* Malini is a very special person, and I'm happy to help you learn more about her. *nodding* Malini is a friend of Arjun's, and they have a very special bond. *giving a gentle pat on the arm* It's clear that they care deeply for each other, and I'm sure their friendship brings a lot of joy to both of them. *smiling warmly* Would you like to know more about their friendship?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(prompt)\n",
    "response = f\"\"\"\n",
    "    answer: {row[\"answers\"]}\n",
    "    prediction: {outputs[0][\"generated_text\"]}\n",
    "\"\"\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54a5070f-ca2f-4f28-928c-dbaf06122392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset,  4.96s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:48<00:00,  4.84s/it]\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for row in tqdm(dataset['test']):\n",
    "    prompt = create_test_prompt(row)\n",
    "    outputs = pipe(prompt)\n",
    "    rows.append(\n",
    "        {\n",
    "            \"question\":row['questions'],\n",
    "            \"prompt\" : prompt,\n",
    "            \"answer\" : row['answers'],\n",
    "            \"untrained_prediction\" : outputs[0]['generated_text'],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e76a758f-bf9d-4900-a660-4cd0526a07e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"<|end_head_id|>\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "examples = [dataset['train'][0]['text']]\n",
    "encodings = [tokenizer(e) for e in examples]\n",
    "\n",
    "dataloader = DataLoader(encodings, collate_fn = collator, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bae54a7f-70cf-488a-beb5-3d2f6c5b1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(dataloader))\n",
    "# batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57dea1-8797-44b1-b9bf-2b921fd35b62",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a36e889e-2350-4a6c-bc94-da7ad2de9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r= 16,\n",
    "    lora_alpha=32,\n",
    "    target_modules = [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\"],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = 'none',\n",
    "    task_type = 'CAUSAL_LM'\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a835ce6f-3027-4182-ad74-3ba58e50ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _,param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "\n",
    "    print(f\"Trainable params : {trainable_params} || All params : {all_param} || Trainable % : {100 * trainable_params}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4672586-fe02-4e1c-8880-95d6097b468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params : 16777216 || All params : 3517190144 || Trainable % : 1677721600\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f8e899f-4a6d-418a-bc17-b6e62b024117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b82fa2d2d34557b0b6aa4dad1f2c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467da29a07644817a55134a967960c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(tokenize, batched=True)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f63a7db6-3472-4b61-bbd2-a52d96d63a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d7aa095273434394eb64454331451a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/189 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e91af1e0d447bc9659a6b7520e487e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"llm_output\"\n",
    "sft_config = SFTConfig(\n",
    "    output_dir = OUTPUT_DIR,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    eval_strategy='steps',\n",
    "    eval_steps = 0.2,\n",
    "    save_steps = 0.2,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True, # or bf16=True,\n",
    "    save_strategy= 'steps',\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=2,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    save_safetensors=True,\n",
    "    dataset_kwargs={\n",
    "        \" add _ special _ tokens\" : False,\n",
    "        \" append_concat_token\" :False\n",
    "    },\n",
    "    seed = 42\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset=dataset [\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e26de8d-daec-4ee5-bcf9-6178e6070758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 02:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.148315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.295700</td>\n",
       "      <td>2.416656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.295700</td>\n",
       "      <td>1.876301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.905000</td>\n",
       "      <td>1.451479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=23, training_loss=2.4367848479229473, metrics={'train_runtime': 145.8571, 'train_samples_per_second': 1.296, 'train_steps_per_second': 0.158, 'total_flos': 799186930139136.0, 'train_loss': 2.4367848479229473})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ca5ba55-881a-48eb-9b3d-669a790a572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_MODEL = 'finetuned_llama_medicare'\n",
    "trainer.save_model(NEW_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22f22ff6-2627-4e9d-ba5a-7475e8f34115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc5d4de181b4f1f84d89b3941c2d254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.1.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.1.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.1.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.1.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.1.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.1.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.1.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.1.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.2.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.2.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.2.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.2.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.2.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.2.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.2.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.2.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.3.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.3.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.3.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.3.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.3.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.3.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.3.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.3.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.4.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.4.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.4.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.4.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.4.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.4.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.4.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.4.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.5.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.5.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.5.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.5.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.5.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.5.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.5.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.5.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.6.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.6.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.6.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.6.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.6.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.6.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.6.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.6.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.7.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.7.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.7.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.7.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.7.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.7.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.7.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.7.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.8.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.8.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.8.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.8.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.8.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.8.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.8.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.8.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.9.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.9.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.9.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.9.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.9.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.9.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.9.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.9.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.10.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.10.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.10.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.10.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.10.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.10.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.10.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.10.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.11.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.11.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.11.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.11.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.11.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.11.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.11.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.11.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.12.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.12.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.12.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.12.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.12.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.12.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.12.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.12.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.13.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.13.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.13.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.13.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.13.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.13.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.13.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.13.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.14.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.14.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.14.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.14.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.14.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.14.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.14.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.14.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.15.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.15.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.15.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.15.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.15.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.15.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.15.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.15.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.16.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.16.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.16.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.16.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.16.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.16.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.16.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.16.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.17.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.17.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.17.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.17.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.17.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.17.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.17.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.17.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.18.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.18.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.18.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.18.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.18.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.18.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.18.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.18.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.19.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.19.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.19.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.19.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.19.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.19.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.19.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.19.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.20.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.20.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.20.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.20.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.20.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.20.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.20.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.20.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.21.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.21.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.21.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.21.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.21.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.21.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.21.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.21.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.22.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.22.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.22.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.22.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.22.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.22.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.22.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.22.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.23.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.23.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.23.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.23.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.23.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.23.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.23.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.23.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.24.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.24.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.24.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.24.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.24.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.24.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.24.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.24.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.25.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.25.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.25.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.25.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.25.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.25.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.25.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.25.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.26.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.26.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.26.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.26.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.26.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.26.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.26.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.26.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.27.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.27.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.27.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.27.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.27.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.27.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.27.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.27.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.28.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.28.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.28.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.28.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.28.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.28.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.28.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.28.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.29.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.29.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.29.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.29.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.29.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.29.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.29.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.29.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.30.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.30.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.30.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.30.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.30.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.30.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.30.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.30.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.31.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.31.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.31.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.31.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.31.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.31.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.31.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/nn/modules/module.py:2409: UserWarning: for model.layers.31.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules need to be offloaded: model.layers.9, model.layers.10, model.layers.11, model.layers.12, model.layers.13, model.layers.14, model.layers.15, model.layers.16, model.layers.17, model.layers.18, model.layers.19, model.layers.20, model.layers.21, model.layers.22, model.layers.23, model.layers.24, model.layers.25, model.layers.26, model.layers.27, model.layers.28, model.layers.29, model.layers.30, model.layers.31, model.norm, model.rotary_emb, lm_head.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mNEW_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./offload\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m model.resize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer), pad_to_multiple_of = \u001b[32m8\u001b[39m)\n\u001b[32m     11\u001b[39m model = PeftModel.from_pretrained(model, NEW_MODEL)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/transformers/modeling_utils.py:4482\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4479\u001b[39m     model.hf_quantizer = hf_quantizer\n\u001b[32m   4481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4482\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4483\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_adapter_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4484\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4486\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4487\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4489\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[32m   4490\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/transformers/integrations/peft.py:276\u001b[39m, in \u001b[36mPeftAdapterMixin.load_adapter\u001b[39m\u001b[34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, low_cpu_mem_usage, is_trainable, adapter_kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# Re-dispatch model and hooks in case the model is offloaded to CPU / Disk.\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    272\u001b[39m     (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhf_device_map\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    273\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m.hf_device_map.values()).intersection({\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m})) > \u001b[32m0\u001b[39m)\n\u001b[32m    274\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.peft_config) == \u001b[32m1\u001b[39m\n\u001b[32m    275\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dispatch_accelerate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/transformers/integrations/peft.py:527\u001b[39m, in \u001b[36mPeftAdapterMixin._dispatch_accelerate_model\u001b[39m\u001b[34m(self, device_map, max_memory, offload_folder, offload_index)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    524\u001b[39m     device_map = infer_auto_device_map(\n\u001b[32m    525\u001b[39m         \u001b[38;5;28mself\u001b[39m, max_memory=max_memory, no_split_module_classes=no_split_module_classes\n\u001b[32m    526\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdispatch_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/accelerate/big_modeling.py:380\u001b[39m, in \u001b[36mdispatch_model\u001b[39m\u001b[34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[39m\n\u001b[32m    378\u001b[39m disk_modules = [name \u001b[38;5;28;01mfor\u001b[39;00m name, device \u001b[38;5;129;01min\u001b[39;00m device_map.items() \u001b[38;5;28;01mif\u001b[39;00m device == \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m offload_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m offload_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(disk_modules) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    381\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWe need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    382\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mneed to be offloaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(disk_modules)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    383\u001b[39m     )\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mlen\u001b[39m(disk_modules) > \u001b[32m0\u001b[39m\n\u001b[32m    386\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m offload_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    387\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(offload_dir) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(os.path.join(offload_dir, \u001b[33m\"\u001b[39m\u001b[33mindex.json\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m    388\u001b[39m ):\n\u001b[32m    389\u001b[39m     disk_state_dict = extract_submodules_state_dict(model.state_dict(), disk_modules)\n",
      "\u001b[31mValueError\u001b[39m: We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules need to be offloaded: model.layers.9, model.layers.10, model.layers.11, model.layers.12, model.layers.13, model.layers.14, model.layers.15, model.layers.16, model.layers.17, model.layers.18, model.layers.19, model.layers.20, model.layers.21, model.layers.22, model.layers.23, model.layers.24, model.layers.25, model.layers.26, model.layers.27, model.layers.28, model.layers.29, model.layers.30, model.layers.31, model.norm, model.rotary_emb, lm_head."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    NEW_MODEL,\n",
    "    torch_dtype = torch.float16,\n",
    "    offload_folder = \"./offload\",\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of = 8)\n",
    "model = PeftModel.from_pretrained(model, NEW_MODEL)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6326da8-ba09-479d-93a7-5a4393323ae0",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0dcc2d5-aa23-457a-84c6-2672fdfe5b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['questions', 'answers', 'text', 'token_count'],\n",
       "        num_rows: 189\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['questions', 'answers', 'text', 'token_count'],\n",
       "        num_rows: 38\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['questions', 'answers', 'text', 'token_count'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"train\" : \"train.json\", \"validation\" : \"val.json\", \"test\" : \"test.json\", }\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6ed4741-ef30-4382-9658-25ae68bd6a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d06dec-7bd2-4226-9cb9-5bf9b7d459f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
