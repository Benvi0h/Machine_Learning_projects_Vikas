{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215e442-4f1a-4e01-ba4f-796a766ce26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# login(token = <token>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29861eb0-00e8-477b-b315-cb963f1b7eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 [00:21<00:00, 10.83s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config = bnb_config, device_map = {\"\":0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9535558-0945-4a4d-a97b-e4790ace6da6",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9460bde-f43d-4e0f-b1fe-9017cc87eabf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prepare_model_for_kbit_training\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m.gradient_checkpointing_enable()\n\u001b[32m      4\u001b[39m model = prepare_model_for_kbit_training(model)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da8d4c2b-b7aa-42c3-8bf1-ae34bde13485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    '''\n",
    "    Prints the number of trainable parameters in model\n",
    "    '''\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _,param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "        print(f\"Trainable parameters : {trainable_params} || All params : {all_param} || Trainable% : {100*trainable_params}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15255b84-a100-4e3f-b00b-faeb771bd9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters : 0 || All params : 131072000 || Trainable% : 0\n",
      "Trainable parameters : 0 || All params : 139460608 || Trainable% : 0\n",
      "Trainable parameters : 32768 || All params : 139493376 || Trainable% : 3276800\n",
      "Trainable parameters : 65536 || All params : 139526144 || Trainable% : 6553600\n",
      "Trainable parameters : 65536 || All params : 147914752 || Trainable% : 6553600\n",
      "Trainable parameters : 98304 || All params : 147947520 || Trainable% : 9830400\n",
      "Trainable parameters : 131072 || All params : 147980288 || Trainable% : 13107200\n",
      "Trainable parameters : 131072 || All params : 156368896 || Trainable% : 13107200\n",
      "Trainable parameters : 163840 || All params : 156401664 || Trainable% : 16384000\n",
      "Trainable parameters : 196608 || All params : 156434432 || Trainable% : 19660800\n",
      "Trainable parameters : 196608 || All params : 164823040 || Trainable% : 19660800\n",
      "Trainable parameters : 229376 || All params : 164855808 || Trainable% : 22937600\n",
      "Trainable parameters : 262144 || All params : 164888576 || Trainable% : 26214400\n",
      "Trainable parameters : 262144 || All params : 187432960 || Trainable% : 26214400\n",
      "Trainable parameters : 262144 || All params : 209977344 || Trainable% : 26214400\n",
      "Trainable parameters : 262144 || All params : 232521728 || Trainable% : 26214400\n",
      "Trainable parameters : 262144 || All params : 232525824 || Trainable% : 26214400\n",
      "Trainable parameters : 262144 || All params : 232529920 || Trainable% : 26214400\n",
      "Trainable parameters : 262144 || All params : 240918528 || Trainable% : 26214400\n",
      "Trainable parameters : 294912 || All params : 240951296 || Trainable% : 29491200\n",
      "Trainable parameters : 327680 || All params : 240984064 || Trainable% : 32768000\n",
      "Trainable parameters : 327680 || All params : 249372672 || Trainable% : 32768000\n",
      "Trainable parameters : 360448 || All params : 249405440 || Trainable% : 36044800\n",
      "Trainable parameters : 393216 || All params : 249438208 || Trainable% : 39321600\n",
      "Trainable parameters : 393216 || All params : 257826816 || Trainable% : 39321600\n",
      "Trainable parameters : 425984 || All params : 257859584 || Trainable% : 42598400\n",
      "Trainable parameters : 458752 || All params : 257892352 || Trainable% : 45875200\n",
      "Trainable parameters : 458752 || All params : 266280960 || Trainable% : 45875200\n",
      "Trainable parameters : 491520 || All params : 266313728 || Trainable% : 49152000\n",
      "Trainable parameters : 524288 || All params : 266346496 || Trainable% : 52428800\n",
      "Trainable parameters : 524288 || All params : 288890880 || Trainable% : 52428800\n",
      "Trainable parameters : 524288 || All params : 311435264 || Trainable% : 52428800\n",
      "Trainable parameters : 524288 || All params : 333979648 || Trainable% : 52428800\n",
      "Trainable parameters : 524288 || All params : 333983744 || Trainable% : 52428800\n",
      "Trainable parameters : 524288 || All params : 333987840 || Trainable% : 52428800\n",
      "Trainable parameters : 524288 || All params : 342376448 || Trainable% : 52428800\n",
      "Trainable parameters : 557056 || All params : 342409216 || Trainable% : 55705600\n",
      "Trainable parameters : 589824 || All params : 342441984 || Trainable% : 58982400\n",
      "Trainable parameters : 589824 || All params : 350830592 || Trainable% : 58982400\n",
      "Trainable parameters : 622592 || All params : 350863360 || Trainable% : 62259200\n",
      "Trainable parameters : 655360 || All params : 350896128 || Trainable% : 65536000\n",
      "Trainable parameters : 655360 || All params : 359284736 || Trainable% : 65536000\n",
      "Trainable parameters : 688128 || All params : 359317504 || Trainable% : 68812800\n",
      "Trainable parameters : 720896 || All params : 359350272 || Trainable% : 72089600\n",
      "Trainable parameters : 720896 || All params : 367738880 || Trainable% : 72089600\n",
      "Trainable parameters : 753664 || All params : 367771648 || Trainable% : 75366400\n",
      "Trainable parameters : 786432 || All params : 367804416 || Trainable% : 78643200\n",
      "Trainable parameters : 786432 || All params : 390348800 || Trainable% : 78643200\n",
      "Trainable parameters : 786432 || All params : 412893184 || Trainable% : 78643200\n",
      "Trainable parameters : 786432 || All params : 435437568 || Trainable% : 78643200\n",
      "Trainable parameters : 786432 || All params : 435441664 || Trainable% : 78643200\n",
      "Trainable parameters : 786432 || All params : 435445760 || Trainable% : 78643200\n",
      "Trainable parameters : 786432 || All params : 443834368 || Trainable% : 78643200\n",
      "Trainable parameters : 819200 || All params : 443867136 || Trainable% : 81920000\n",
      "Trainable parameters : 851968 || All params : 443899904 || Trainable% : 85196800\n",
      "Trainable parameters : 851968 || All params : 452288512 || Trainable% : 85196800\n",
      "Trainable parameters : 884736 || All params : 452321280 || Trainable% : 88473600\n",
      "Trainable parameters : 917504 || All params : 452354048 || Trainable% : 91750400\n",
      "Trainable parameters : 917504 || All params : 460742656 || Trainable% : 91750400\n",
      "Trainable parameters : 950272 || All params : 460775424 || Trainable% : 95027200\n",
      "Trainable parameters : 983040 || All params : 460808192 || Trainable% : 98304000\n",
      "Trainable parameters : 983040 || All params : 469196800 || Trainable% : 98304000\n",
      "Trainable parameters : 1015808 || All params : 469229568 || Trainable% : 101580800\n",
      "Trainable parameters : 1048576 || All params : 469262336 || Trainable% : 104857600\n",
      "Trainable parameters : 1048576 || All params : 491806720 || Trainable% : 104857600\n",
      "Trainable parameters : 1048576 || All params : 514351104 || Trainable% : 104857600\n",
      "Trainable parameters : 1048576 || All params : 536895488 || Trainable% : 104857600\n",
      "Trainable parameters : 1048576 || All params : 536899584 || Trainable% : 104857600\n",
      "Trainable parameters : 1048576 || All params : 536903680 || Trainable% : 104857600\n",
      "Trainable parameters : 1048576 || All params : 545292288 || Trainable% : 104857600\n",
      "Trainable parameters : 1081344 || All params : 545325056 || Trainable% : 108134400\n",
      "Trainable parameters : 1114112 || All params : 545357824 || Trainable% : 111411200\n",
      "Trainable parameters : 1114112 || All params : 553746432 || Trainable% : 111411200\n",
      "Trainable parameters : 1146880 || All params : 553779200 || Trainable% : 114688000\n",
      "Trainable parameters : 1179648 || All params : 553811968 || Trainable% : 117964800\n",
      "Trainable parameters : 1179648 || All params : 562200576 || Trainable% : 117964800\n",
      "Trainable parameters : 1212416 || All params : 562233344 || Trainable% : 121241600\n",
      "Trainable parameters : 1245184 || All params : 562266112 || Trainable% : 124518400\n",
      "Trainable parameters : 1245184 || All params : 570654720 || Trainable% : 124518400\n",
      "Trainable parameters : 1277952 || All params : 570687488 || Trainable% : 127795200\n",
      "Trainable parameters : 1310720 || All params : 570720256 || Trainable% : 131072000\n",
      "Trainable parameters : 1310720 || All params : 593264640 || Trainable% : 131072000\n",
      "Trainable parameters : 1310720 || All params : 615809024 || Trainable% : 131072000\n",
      "Trainable parameters : 1310720 || All params : 638353408 || Trainable% : 131072000\n",
      "Trainable parameters : 1310720 || All params : 638357504 || Trainable% : 131072000\n",
      "Trainable parameters : 1310720 || All params : 638361600 || Trainable% : 131072000\n",
      "Trainable parameters : 1310720 || All params : 646750208 || Trainable% : 131072000\n",
      "Trainable parameters : 1343488 || All params : 646782976 || Trainable% : 134348800\n",
      "Trainable parameters : 1376256 || All params : 646815744 || Trainable% : 137625600\n",
      "Trainable parameters : 1376256 || All params : 655204352 || Trainable% : 137625600\n",
      "Trainable parameters : 1409024 || All params : 655237120 || Trainable% : 140902400\n",
      "Trainable parameters : 1441792 || All params : 655269888 || Trainable% : 144179200\n",
      "Trainable parameters : 1441792 || All params : 663658496 || Trainable% : 144179200\n",
      "Trainable parameters : 1474560 || All params : 663691264 || Trainable% : 147456000\n",
      "Trainable parameters : 1507328 || All params : 663724032 || Trainable% : 150732800\n",
      "Trainable parameters : 1507328 || All params : 672112640 || Trainable% : 150732800\n",
      "Trainable parameters : 1540096 || All params : 672145408 || Trainable% : 154009600\n",
      "Trainable parameters : 1572864 || All params : 672178176 || Trainable% : 157286400\n",
      "Trainable parameters : 1572864 || All params : 694722560 || Trainable% : 157286400\n",
      "Trainable parameters : 1572864 || All params : 717266944 || Trainable% : 157286400\n",
      "Trainable parameters : 1572864 || All params : 739811328 || Trainable% : 157286400\n",
      "Trainable parameters : 1572864 || All params : 739815424 || Trainable% : 157286400\n",
      "Trainable parameters : 1572864 || All params : 739819520 || Trainable% : 157286400\n",
      "Trainable parameters : 1572864 || All params : 748208128 || Trainable% : 157286400\n",
      "Trainable parameters : 1605632 || All params : 748240896 || Trainable% : 160563200\n",
      "Trainable parameters : 1638400 || All params : 748273664 || Trainable% : 163840000\n",
      "Trainable parameters : 1638400 || All params : 756662272 || Trainable% : 163840000\n",
      "Trainable parameters : 1671168 || All params : 756695040 || Trainable% : 167116800\n",
      "Trainable parameters : 1703936 || All params : 756727808 || Trainable% : 170393600\n",
      "Trainable parameters : 1703936 || All params : 765116416 || Trainable% : 170393600\n",
      "Trainable parameters : 1736704 || All params : 765149184 || Trainable% : 173670400\n",
      "Trainable parameters : 1769472 || All params : 765181952 || Trainable% : 176947200\n",
      "Trainable parameters : 1769472 || All params : 773570560 || Trainable% : 176947200\n",
      "Trainable parameters : 1802240 || All params : 773603328 || Trainable% : 180224000\n",
      "Trainable parameters : 1835008 || All params : 773636096 || Trainable% : 183500800\n",
      "Trainable parameters : 1835008 || All params : 796180480 || Trainable% : 183500800\n",
      "Trainable parameters : 1835008 || All params : 818724864 || Trainable% : 183500800\n",
      "Trainable parameters : 1835008 || All params : 841269248 || Trainable% : 183500800\n",
      "Trainable parameters : 1835008 || All params : 841273344 || Trainable% : 183500800\n",
      "Trainable parameters : 1835008 || All params : 841277440 || Trainable% : 183500800\n",
      "Trainable parameters : 1835008 || All params : 849666048 || Trainable% : 183500800\n",
      "Trainable parameters : 1867776 || All params : 849698816 || Trainable% : 186777600\n",
      "Trainable parameters : 1900544 || All params : 849731584 || Trainable% : 190054400\n",
      "Trainable parameters : 1900544 || All params : 858120192 || Trainable% : 190054400\n",
      "Trainable parameters : 1933312 || All params : 858152960 || Trainable% : 193331200\n",
      "Trainable parameters : 1966080 || All params : 858185728 || Trainable% : 196608000\n",
      "Trainable parameters : 1966080 || All params : 866574336 || Trainable% : 196608000\n",
      "Trainable parameters : 1998848 || All params : 866607104 || Trainable% : 199884800\n",
      "Trainable parameters : 2031616 || All params : 866639872 || Trainable% : 203161600\n",
      "Trainable parameters : 2031616 || All params : 875028480 || Trainable% : 203161600\n",
      "Trainable parameters : 2064384 || All params : 875061248 || Trainable% : 206438400\n",
      "Trainable parameters : 2097152 || All params : 875094016 || Trainable% : 209715200\n",
      "Trainable parameters : 2097152 || All params : 897638400 || Trainable% : 209715200\n",
      "Trainable parameters : 2097152 || All params : 920182784 || Trainable% : 209715200\n",
      "Trainable parameters : 2097152 || All params : 942727168 || Trainable% : 209715200\n",
      "Trainable parameters : 2097152 || All params : 942731264 || Trainable% : 209715200\n",
      "Trainable parameters : 2097152 || All params : 942735360 || Trainable% : 209715200\n",
      "Trainable parameters : 2097152 || All params : 951123968 || Trainable% : 209715200\n",
      "Trainable parameters : 2129920 || All params : 951156736 || Trainable% : 212992000\n",
      "Trainable parameters : 2162688 || All params : 951189504 || Trainable% : 216268800\n",
      "Trainable parameters : 2162688 || All params : 959578112 || Trainable% : 216268800\n",
      "Trainable parameters : 2195456 || All params : 959610880 || Trainable% : 219545600\n",
      "Trainable parameters : 2228224 || All params : 959643648 || Trainable% : 222822400\n",
      "Trainable parameters : 2228224 || All params : 968032256 || Trainable% : 222822400\n",
      "Trainable parameters : 2260992 || All params : 968065024 || Trainable% : 226099200\n",
      "Trainable parameters : 2293760 || All params : 968097792 || Trainable% : 229376000\n",
      "Trainable parameters : 2293760 || All params : 976486400 || Trainable% : 229376000\n",
      "Trainable parameters : 2326528 || All params : 976519168 || Trainable% : 232652800\n",
      "Trainable parameters : 2359296 || All params : 976551936 || Trainable% : 235929600\n",
      "Trainable parameters : 2359296 || All params : 999096320 || Trainable% : 235929600\n",
      "Trainable parameters : 2359296 || All params : 1021640704 || Trainable% : 235929600\n",
      "Trainable parameters : 2359296 || All params : 1044185088 || Trainable% : 235929600\n",
      "Trainable parameters : 2359296 || All params : 1044189184 || Trainable% : 235929600\n",
      "Trainable parameters : 2359296 || All params : 1044193280 || Trainable% : 235929600\n",
      "Trainable parameters : 2359296 || All params : 1052581888 || Trainable% : 235929600\n",
      "Trainable parameters : 2392064 || All params : 1052614656 || Trainable% : 239206400\n",
      "Trainable parameters : 2424832 || All params : 1052647424 || Trainable% : 242483200\n",
      "Trainable parameters : 2424832 || All params : 1061036032 || Trainable% : 242483200\n",
      "Trainable parameters : 2457600 || All params : 1061068800 || Trainable% : 245760000\n",
      "Trainable parameters : 2490368 || All params : 1061101568 || Trainable% : 249036800\n",
      "Trainable parameters : 2490368 || All params : 1069490176 || Trainable% : 249036800\n",
      "Trainable parameters : 2523136 || All params : 1069522944 || Trainable% : 252313600\n",
      "Trainable parameters : 2555904 || All params : 1069555712 || Trainable% : 255590400\n",
      "Trainable parameters : 2555904 || All params : 1077944320 || Trainable% : 255590400\n",
      "Trainable parameters : 2588672 || All params : 1077977088 || Trainable% : 258867200\n",
      "Trainable parameters : 2621440 || All params : 1078009856 || Trainable% : 262144000\n",
      "Trainable parameters : 2621440 || All params : 1100554240 || Trainable% : 262144000\n",
      "Trainable parameters : 2621440 || All params : 1123098624 || Trainable% : 262144000\n",
      "Trainable parameters : 2621440 || All params : 1145643008 || Trainable% : 262144000\n",
      "Trainable parameters : 2621440 || All params : 1145647104 || Trainable% : 262144000\n",
      "Trainable parameters : 2621440 || All params : 1145651200 || Trainable% : 262144000\n",
      "Trainable parameters : 2621440 || All params : 1154039808 || Trainable% : 262144000\n",
      "Trainable parameters : 2654208 || All params : 1154072576 || Trainable% : 265420800\n",
      "Trainable parameters : 2686976 || All params : 1154105344 || Trainable% : 268697600\n",
      "Trainable parameters : 2686976 || All params : 1162493952 || Trainable% : 268697600\n",
      "Trainable parameters : 2719744 || All params : 1162526720 || Trainable% : 271974400\n",
      "Trainable parameters : 2752512 || All params : 1162559488 || Trainable% : 275251200\n",
      "Trainable parameters : 2752512 || All params : 1170948096 || Trainable% : 275251200\n",
      "Trainable parameters : 2785280 || All params : 1170980864 || Trainable% : 278528000\n",
      "Trainable parameters : 2818048 || All params : 1171013632 || Trainable% : 281804800\n",
      "Trainable parameters : 2818048 || All params : 1179402240 || Trainable% : 281804800\n",
      "Trainable parameters : 2850816 || All params : 1179435008 || Trainable% : 285081600\n",
      "Trainable parameters : 2883584 || All params : 1179467776 || Trainable% : 288358400\n",
      "Trainable parameters : 2883584 || All params : 1202012160 || Trainable% : 288358400\n",
      "Trainable parameters : 2883584 || All params : 1224556544 || Trainable% : 288358400\n",
      "Trainable parameters : 2883584 || All params : 1247100928 || Trainable% : 288358400\n",
      "Trainable parameters : 2883584 || All params : 1247105024 || Trainable% : 288358400\n",
      "Trainable parameters : 2883584 || All params : 1247109120 || Trainable% : 288358400\n",
      "Trainable parameters : 2883584 || All params : 1255497728 || Trainable% : 288358400\n",
      "Trainable parameters : 2916352 || All params : 1255530496 || Trainable% : 291635200\n",
      "Trainable parameters : 2949120 || All params : 1255563264 || Trainable% : 294912000\n",
      "Trainable parameters : 2949120 || All params : 1263951872 || Trainable% : 294912000\n",
      "Trainable parameters : 2981888 || All params : 1263984640 || Trainable% : 298188800\n",
      "Trainable parameters : 3014656 || All params : 1264017408 || Trainable% : 301465600\n",
      "Trainable parameters : 3014656 || All params : 1272406016 || Trainable% : 301465600\n",
      "Trainable parameters : 3047424 || All params : 1272438784 || Trainable% : 304742400\n",
      "Trainable parameters : 3080192 || All params : 1272471552 || Trainable% : 308019200\n",
      "Trainable parameters : 3080192 || All params : 1280860160 || Trainable% : 308019200\n",
      "Trainable parameters : 3112960 || All params : 1280892928 || Trainable% : 311296000\n",
      "Trainable parameters : 3145728 || All params : 1280925696 || Trainable% : 314572800\n",
      "Trainable parameters : 3145728 || All params : 1303470080 || Trainable% : 314572800\n",
      "Trainable parameters : 3145728 || All params : 1326014464 || Trainable% : 314572800\n",
      "Trainable parameters : 3145728 || All params : 1348558848 || Trainable% : 314572800\n",
      "Trainable parameters : 3145728 || All params : 1348562944 || Trainable% : 314572800\n",
      "Trainable parameters : 3145728 || All params : 1348567040 || Trainable% : 314572800\n",
      "Trainable parameters : 3145728 || All params : 1356955648 || Trainable% : 314572800\n",
      "Trainable parameters : 3178496 || All params : 1356988416 || Trainable% : 317849600\n",
      "Trainable parameters : 3211264 || All params : 1357021184 || Trainable% : 321126400\n",
      "Trainable parameters : 3211264 || All params : 1365409792 || Trainable% : 321126400\n",
      "Trainable parameters : 3244032 || All params : 1365442560 || Trainable% : 324403200\n",
      "Trainable parameters : 3276800 || All params : 1365475328 || Trainable% : 327680000\n",
      "Trainable parameters : 3276800 || All params : 1373863936 || Trainable% : 327680000\n",
      "Trainable parameters : 3309568 || All params : 1373896704 || Trainable% : 330956800\n",
      "Trainable parameters : 3342336 || All params : 1373929472 || Trainable% : 334233600\n",
      "Trainable parameters : 3342336 || All params : 1382318080 || Trainable% : 334233600\n",
      "Trainable parameters : 3375104 || All params : 1382350848 || Trainable% : 337510400\n",
      "Trainable parameters : 3407872 || All params : 1382383616 || Trainable% : 340787200\n",
      "Trainable parameters : 3407872 || All params : 1404928000 || Trainable% : 340787200\n",
      "Trainable parameters : 3407872 || All params : 1427472384 || Trainable% : 340787200\n",
      "Trainable parameters : 3407872 || All params : 1450016768 || Trainable% : 340787200\n",
      "Trainable parameters : 3407872 || All params : 1450020864 || Trainable% : 340787200\n",
      "Trainable parameters : 3407872 || All params : 1450024960 || Trainable% : 340787200\n",
      "Trainable parameters : 3407872 || All params : 1458413568 || Trainable% : 340787200\n",
      "Trainable parameters : 3440640 || All params : 1458446336 || Trainable% : 344064000\n",
      "Trainable parameters : 3473408 || All params : 1458479104 || Trainable% : 347340800\n",
      "Trainable parameters : 3473408 || All params : 1466867712 || Trainable% : 347340800\n",
      "Trainable parameters : 3506176 || All params : 1466900480 || Trainable% : 350617600\n",
      "Trainable parameters : 3538944 || All params : 1466933248 || Trainable% : 353894400\n",
      "Trainable parameters : 3538944 || All params : 1475321856 || Trainable% : 353894400\n",
      "Trainable parameters : 3571712 || All params : 1475354624 || Trainable% : 357171200\n",
      "Trainable parameters : 3604480 || All params : 1475387392 || Trainable% : 360448000\n",
      "Trainable parameters : 3604480 || All params : 1483776000 || Trainable% : 360448000\n",
      "Trainable parameters : 3637248 || All params : 1483808768 || Trainable% : 363724800\n",
      "Trainable parameters : 3670016 || All params : 1483841536 || Trainable% : 367001600\n",
      "Trainable parameters : 3670016 || All params : 1506385920 || Trainable% : 367001600\n",
      "Trainable parameters : 3670016 || All params : 1528930304 || Trainable% : 367001600\n",
      "Trainable parameters : 3670016 || All params : 1551474688 || Trainable% : 367001600\n",
      "Trainable parameters : 3670016 || All params : 1551478784 || Trainable% : 367001600\n",
      "Trainable parameters : 3670016 || All params : 1551482880 || Trainable% : 367001600\n",
      "Trainable parameters : 3670016 || All params : 1559871488 || Trainable% : 367001600\n",
      "Trainable parameters : 3702784 || All params : 1559904256 || Trainable% : 370278400\n",
      "Trainable parameters : 3735552 || All params : 1559937024 || Trainable% : 373555200\n",
      "Trainable parameters : 3735552 || All params : 1568325632 || Trainable% : 373555200\n",
      "Trainable parameters : 3768320 || All params : 1568358400 || Trainable% : 376832000\n",
      "Trainable parameters : 3801088 || All params : 1568391168 || Trainable% : 380108800\n",
      "Trainable parameters : 3801088 || All params : 1576779776 || Trainable% : 380108800\n",
      "Trainable parameters : 3833856 || All params : 1576812544 || Trainable% : 383385600\n",
      "Trainable parameters : 3866624 || All params : 1576845312 || Trainable% : 386662400\n",
      "Trainable parameters : 3866624 || All params : 1585233920 || Trainable% : 386662400\n",
      "Trainable parameters : 3899392 || All params : 1585266688 || Trainable% : 389939200\n",
      "Trainable parameters : 3932160 || All params : 1585299456 || Trainable% : 393216000\n",
      "Trainable parameters : 3932160 || All params : 1607843840 || Trainable% : 393216000\n",
      "Trainable parameters : 3932160 || All params : 1630388224 || Trainable% : 393216000\n",
      "Trainable parameters : 3932160 || All params : 1652932608 || Trainable% : 393216000\n",
      "Trainable parameters : 3932160 || All params : 1652936704 || Trainable% : 393216000\n",
      "Trainable parameters : 3932160 || All params : 1652940800 || Trainable% : 393216000\n",
      "Trainable parameters : 3932160 || All params : 1661329408 || Trainable% : 393216000\n",
      "Trainable parameters : 3964928 || All params : 1661362176 || Trainable% : 396492800\n",
      "Trainable parameters : 3997696 || All params : 1661394944 || Trainable% : 399769600\n",
      "Trainable parameters : 3997696 || All params : 1669783552 || Trainable% : 399769600\n",
      "Trainable parameters : 4030464 || All params : 1669816320 || Trainable% : 403046400\n",
      "Trainable parameters : 4063232 || All params : 1669849088 || Trainable% : 406323200\n",
      "Trainable parameters : 4063232 || All params : 1678237696 || Trainable% : 406323200\n",
      "Trainable parameters : 4096000 || All params : 1678270464 || Trainable% : 409600000\n",
      "Trainable parameters : 4128768 || All params : 1678303232 || Trainable% : 412876800\n",
      "Trainable parameters : 4128768 || All params : 1686691840 || Trainable% : 412876800\n",
      "Trainable parameters : 4161536 || All params : 1686724608 || Trainable% : 416153600\n",
      "Trainable parameters : 4194304 || All params : 1686757376 || Trainable% : 419430400\n",
      "Trainable parameters : 4194304 || All params : 1709301760 || Trainable% : 419430400\n",
      "Trainable parameters : 4194304 || All params : 1731846144 || Trainable% : 419430400\n",
      "Trainable parameters : 4194304 || All params : 1754390528 || Trainable% : 419430400\n",
      "Trainable parameters : 4194304 || All params : 1754394624 || Trainable% : 419430400\n",
      "Trainable parameters : 4194304 || All params : 1754398720 || Trainable% : 419430400\n",
      "Trainable parameters : 4194304 || All params : 1762787328 || Trainable% : 419430400\n",
      "Trainable parameters : 4227072 || All params : 1762820096 || Trainable% : 422707200\n",
      "Trainable parameters : 4259840 || All params : 1762852864 || Trainable% : 425984000\n",
      "Trainable parameters : 4259840 || All params : 1771241472 || Trainable% : 425984000\n",
      "Trainable parameters : 4292608 || All params : 1771274240 || Trainable% : 429260800\n",
      "Trainable parameters : 4325376 || All params : 1771307008 || Trainable% : 432537600\n",
      "Trainable parameters : 4325376 || All params : 1779695616 || Trainable% : 432537600\n",
      "Trainable parameters : 4358144 || All params : 1779728384 || Trainable% : 435814400\n",
      "Trainable parameters : 4390912 || All params : 1779761152 || Trainable% : 439091200\n",
      "Trainable parameters : 4390912 || All params : 1788149760 || Trainable% : 439091200\n",
      "Trainable parameters : 4423680 || All params : 1788182528 || Trainable% : 442368000\n",
      "Trainable parameters : 4456448 || All params : 1788215296 || Trainable% : 445644800\n",
      "Trainable parameters : 4456448 || All params : 1810759680 || Trainable% : 445644800\n",
      "Trainable parameters : 4456448 || All params : 1833304064 || Trainable% : 445644800\n",
      "Trainable parameters : 4456448 || All params : 1855848448 || Trainable% : 445644800\n",
      "Trainable parameters : 4456448 || All params : 1855852544 || Trainable% : 445644800\n",
      "Trainable parameters : 4456448 || All params : 1855856640 || Trainable% : 445644800\n",
      "Trainable parameters : 4456448 || All params : 1864245248 || Trainable% : 445644800\n",
      "Trainable parameters : 4489216 || All params : 1864278016 || Trainable% : 448921600\n",
      "Trainable parameters : 4521984 || All params : 1864310784 || Trainable% : 452198400\n",
      "Trainable parameters : 4521984 || All params : 1872699392 || Trainable% : 452198400\n",
      "Trainable parameters : 4554752 || All params : 1872732160 || Trainable% : 455475200\n",
      "Trainable parameters : 4587520 || All params : 1872764928 || Trainable% : 458752000\n",
      "Trainable parameters : 4587520 || All params : 1881153536 || Trainable% : 458752000\n",
      "Trainable parameters : 4620288 || All params : 1881186304 || Trainable% : 462028800\n",
      "Trainable parameters : 4653056 || All params : 1881219072 || Trainable% : 465305600\n",
      "Trainable parameters : 4653056 || All params : 1889607680 || Trainable% : 465305600\n",
      "Trainable parameters : 4685824 || All params : 1889640448 || Trainable% : 468582400\n",
      "Trainable parameters : 4718592 || All params : 1889673216 || Trainable% : 471859200\n",
      "Trainable parameters : 4718592 || All params : 1912217600 || Trainable% : 471859200\n",
      "Trainable parameters : 4718592 || All params : 1934761984 || Trainable% : 471859200\n",
      "Trainable parameters : 4718592 || All params : 1957306368 || Trainable% : 471859200\n",
      "Trainable parameters : 4718592 || All params : 1957310464 || Trainable% : 471859200\n",
      "Trainable parameters : 4718592 || All params : 1957314560 || Trainable% : 471859200\n",
      "Trainable parameters : 4718592 || All params : 1965703168 || Trainable% : 471859200\n",
      "Trainable parameters : 4751360 || All params : 1965735936 || Trainable% : 475136000\n",
      "Trainable parameters : 4784128 || All params : 1965768704 || Trainable% : 478412800\n",
      "Trainable parameters : 4784128 || All params : 1974157312 || Trainable% : 478412800\n",
      "Trainable parameters : 4816896 || All params : 1974190080 || Trainable% : 481689600\n",
      "Trainable parameters : 4849664 || All params : 1974222848 || Trainable% : 484966400\n",
      "Trainable parameters : 4849664 || All params : 1982611456 || Trainable% : 484966400\n",
      "Trainable parameters : 4882432 || All params : 1982644224 || Trainable% : 488243200\n",
      "Trainable parameters : 4915200 || All params : 1982676992 || Trainable% : 491520000\n",
      "Trainable parameters : 4915200 || All params : 1991065600 || Trainable% : 491520000\n",
      "Trainable parameters : 4947968 || All params : 1991098368 || Trainable% : 494796800\n",
      "Trainable parameters : 4980736 || All params : 1991131136 || Trainable% : 498073600\n",
      "Trainable parameters : 4980736 || All params : 2013675520 || Trainable% : 498073600\n",
      "Trainable parameters : 4980736 || All params : 2036219904 || Trainable% : 498073600\n",
      "Trainable parameters : 4980736 || All params : 2058764288 || Trainable% : 498073600\n",
      "Trainable parameters : 4980736 || All params : 2058768384 || Trainable% : 498073600\n",
      "Trainable parameters : 4980736 || All params : 2058772480 || Trainable% : 498073600\n",
      "Trainable parameters : 4980736 || All params : 2067161088 || Trainable% : 498073600\n",
      "Trainable parameters : 5013504 || All params : 2067193856 || Trainable% : 501350400\n",
      "Trainable parameters : 5046272 || All params : 2067226624 || Trainable% : 504627200\n",
      "Trainable parameters : 5046272 || All params : 2075615232 || Trainable% : 504627200\n",
      "Trainable parameters : 5079040 || All params : 2075648000 || Trainable% : 507904000\n",
      "Trainable parameters : 5111808 || All params : 2075680768 || Trainable% : 511180800\n",
      "Trainable parameters : 5111808 || All params : 2084069376 || Trainable% : 511180800\n",
      "Trainable parameters : 5144576 || All params : 2084102144 || Trainable% : 514457600\n",
      "Trainable parameters : 5177344 || All params : 2084134912 || Trainable% : 517734400\n",
      "Trainable parameters : 5177344 || All params : 2092523520 || Trainable% : 517734400\n",
      "Trainable parameters : 5210112 || All params : 2092556288 || Trainable% : 521011200\n",
      "Trainable parameters : 5242880 || All params : 2092589056 || Trainable% : 524288000\n",
      "Trainable parameters : 5242880 || All params : 2115133440 || Trainable% : 524288000\n",
      "Trainable parameters : 5242880 || All params : 2137677824 || Trainable% : 524288000\n",
      "Trainable parameters : 5242880 || All params : 2160222208 || Trainable% : 524288000\n",
      "Trainable parameters : 5242880 || All params : 2160226304 || Trainable% : 524288000\n",
      "Trainable parameters : 5242880 || All params : 2160230400 || Trainable% : 524288000\n",
      "Trainable parameters : 5242880 || All params : 2168619008 || Trainable% : 524288000\n",
      "Trainable parameters : 5275648 || All params : 2168651776 || Trainable% : 527564800\n",
      "Trainable parameters : 5308416 || All params : 2168684544 || Trainable% : 530841600\n",
      "Trainable parameters : 5308416 || All params : 2177073152 || Trainable% : 530841600\n",
      "Trainable parameters : 5341184 || All params : 2177105920 || Trainable% : 534118400\n",
      "Trainable parameters : 5373952 || All params : 2177138688 || Trainable% : 537395200\n",
      "Trainable parameters : 5373952 || All params : 2185527296 || Trainable% : 537395200\n",
      "Trainable parameters : 5406720 || All params : 2185560064 || Trainable% : 540672000\n",
      "Trainable parameters : 5439488 || All params : 2185592832 || Trainable% : 543948800\n",
      "Trainable parameters : 5439488 || All params : 2193981440 || Trainable% : 543948800\n",
      "Trainable parameters : 5472256 || All params : 2194014208 || Trainable% : 547225600\n",
      "Trainable parameters : 5505024 || All params : 2194046976 || Trainable% : 550502400\n",
      "Trainable parameters : 5505024 || All params : 2216591360 || Trainable% : 550502400\n",
      "Trainable parameters : 5505024 || All params : 2239135744 || Trainable% : 550502400\n",
      "Trainable parameters : 5505024 || All params : 2261680128 || Trainable% : 550502400\n",
      "Trainable parameters : 5505024 || All params : 2261684224 || Trainable% : 550502400\n",
      "Trainable parameters : 5505024 || All params : 2261688320 || Trainable% : 550502400\n",
      "Trainable parameters : 5505024 || All params : 2270076928 || Trainable% : 550502400\n",
      "Trainable parameters : 5537792 || All params : 2270109696 || Trainable% : 553779200\n",
      "Trainable parameters : 5570560 || All params : 2270142464 || Trainable% : 557056000\n",
      "Trainable parameters : 5570560 || All params : 2278531072 || Trainable% : 557056000\n",
      "Trainable parameters : 5603328 || All params : 2278563840 || Trainable% : 560332800\n",
      "Trainable parameters : 5636096 || All params : 2278596608 || Trainable% : 563609600\n",
      "Trainable parameters : 5636096 || All params : 2286985216 || Trainable% : 563609600\n",
      "Trainable parameters : 5668864 || All params : 2287017984 || Trainable% : 566886400\n",
      "Trainable parameters : 5701632 || All params : 2287050752 || Trainable% : 570163200\n",
      "Trainable parameters : 5701632 || All params : 2295439360 || Trainable% : 570163200\n",
      "Trainable parameters : 5734400 || All params : 2295472128 || Trainable% : 573440000\n",
      "Trainable parameters : 5767168 || All params : 2295504896 || Trainable% : 576716800\n",
      "Trainable parameters : 5767168 || All params : 2318049280 || Trainable% : 576716800\n",
      "Trainable parameters : 5767168 || All params : 2340593664 || Trainable% : 576716800\n",
      "Trainable parameters : 5767168 || All params : 2363138048 || Trainable% : 576716800\n",
      "Trainable parameters : 5767168 || All params : 2363142144 || Trainable% : 576716800\n",
      "Trainable parameters : 5767168 || All params : 2363146240 || Trainable% : 576716800\n",
      "Trainable parameters : 5767168 || All params : 2371534848 || Trainable% : 576716800\n",
      "Trainable parameters : 5799936 || All params : 2371567616 || Trainable% : 579993600\n",
      "Trainable parameters : 5832704 || All params : 2371600384 || Trainable% : 583270400\n",
      "Trainable parameters : 5832704 || All params : 2379988992 || Trainable% : 583270400\n",
      "Trainable parameters : 5865472 || All params : 2380021760 || Trainable% : 586547200\n",
      "Trainable parameters : 5898240 || All params : 2380054528 || Trainable% : 589824000\n",
      "Trainable parameters : 5898240 || All params : 2388443136 || Trainable% : 589824000\n",
      "Trainable parameters : 5931008 || All params : 2388475904 || Trainable% : 593100800\n",
      "Trainable parameters : 5963776 || All params : 2388508672 || Trainable% : 596377600\n",
      "Trainable parameters : 5963776 || All params : 2396897280 || Trainable% : 596377600\n",
      "Trainable parameters : 5996544 || All params : 2396930048 || Trainable% : 599654400\n",
      "Trainable parameters : 6029312 || All params : 2396962816 || Trainable% : 602931200\n",
      "Trainable parameters : 6029312 || All params : 2419507200 || Trainable% : 602931200\n",
      "Trainable parameters : 6029312 || All params : 2442051584 || Trainable% : 602931200\n",
      "Trainable parameters : 6029312 || All params : 2464595968 || Trainable% : 602931200\n",
      "Trainable parameters : 6029312 || All params : 2464600064 || Trainable% : 602931200\n",
      "Trainable parameters : 6029312 || All params : 2464604160 || Trainable% : 602931200\n",
      "Trainable parameters : 6029312 || All params : 2472992768 || Trainable% : 602931200\n",
      "Trainable parameters : 6062080 || All params : 2473025536 || Trainable% : 606208000\n",
      "Trainable parameters : 6094848 || All params : 2473058304 || Trainable% : 609484800\n",
      "Trainable parameters : 6094848 || All params : 2481446912 || Trainable% : 609484800\n",
      "Trainable parameters : 6127616 || All params : 2481479680 || Trainable% : 612761600\n",
      "Trainable parameters : 6160384 || All params : 2481512448 || Trainable% : 616038400\n",
      "Trainable parameters : 6160384 || All params : 2489901056 || Trainable% : 616038400\n",
      "Trainable parameters : 6193152 || All params : 2489933824 || Trainable% : 619315200\n",
      "Trainable parameters : 6225920 || All params : 2489966592 || Trainable% : 622592000\n",
      "Trainable parameters : 6225920 || All params : 2498355200 || Trainable% : 622592000\n",
      "Trainable parameters : 6258688 || All params : 2498387968 || Trainable% : 625868800\n",
      "Trainable parameters : 6291456 || All params : 2498420736 || Trainable% : 629145600\n",
      "Trainable parameters : 6291456 || All params : 2520965120 || Trainable% : 629145600\n",
      "Trainable parameters : 6291456 || All params : 2543509504 || Trainable% : 629145600\n",
      "Trainable parameters : 6291456 || All params : 2566053888 || Trainable% : 629145600\n",
      "Trainable parameters : 6291456 || All params : 2566057984 || Trainable% : 629145600\n",
      "Trainable parameters : 6291456 || All params : 2566062080 || Trainable% : 629145600\n",
      "Trainable parameters : 6291456 || All params : 2574450688 || Trainable% : 629145600\n",
      "Trainable parameters : 6324224 || All params : 2574483456 || Trainable% : 632422400\n",
      "Trainable parameters : 6356992 || All params : 2574516224 || Trainable% : 635699200\n",
      "Trainable parameters : 6356992 || All params : 2582904832 || Trainable% : 635699200\n",
      "Trainable parameters : 6389760 || All params : 2582937600 || Trainable% : 638976000\n",
      "Trainable parameters : 6422528 || All params : 2582970368 || Trainable% : 642252800\n",
      "Trainable parameters : 6422528 || All params : 2591358976 || Trainable% : 642252800\n",
      "Trainable parameters : 6455296 || All params : 2591391744 || Trainable% : 645529600\n",
      "Trainable parameters : 6488064 || All params : 2591424512 || Trainable% : 648806400\n",
      "Trainable parameters : 6488064 || All params : 2599813120 || Trainable% : 648806400\n",
      "Trainable parameters : 6520832 || All params : 2599845888 || Trainable% : 652083200\n",
      "Trainable parameters : 6553600 || All params : 2599878656 || Trainable% : 655360000\n",
      "Trainable parameters : 6553600 || All params : 2622423040 || Trainable% : 655360000\n",
      "Trainable parameters : 6553600 || All params : 2644967424 || Trainable% : 655360000\n",
      "Trainable parameters : 6553600 || All params : 2667511808 || Trainable% : 655360000\n",
      "Trainable parameters : 6553600 || All params : 2667515904 || Trainable% : 655360000\n",
      "Trainable parameters : 6553600 || All params : 2667520000 || Trainable% : 655360000\n",
      "Trainable parameters : 6553600 || All params : 2675908608 || Trainable% : 655360000\n",
      "Trainable parameters : 6586368 || All params : 2675941376 || Trainable% : 658636800\n",
      "Trainable parameters : 6619136 || All params : 2675974144 || Trainable% : 661913600\n",
      "Trainable parameters : 6619136 || All params : 2684362752 || Trainable% : 661913600\n",
      "Trainable parameters : 6651904 || All params : 2684395520 || Trainable% : 665190400\n",
      "Trainable parameters : 6684672 || All params : 2684428288 || Trainable% : 668467200\n",
      "Trainable parameters : 6684672 || All params : 2692816896 || Trainable% : 668467200\n",
      "Trainable parameters : 6717440 || All params : 2692849664 || Trainable% : 671744000\n",
      "Trainable parameters : 6750208 || All params : 2692882432 || Trainable% : 675020800\n",
      "Trainable parameters : 6750208 || All params : 2701271040 || Trainable% : 675020800\n",
      "Trainable parameters : 6782976 || All params : 2701303808 || Trainable% : 678297600\n",
      "Trainable parameters : 6815744 || All params : 2701336576 || Trainable% : 681574400\n",
      "Trainable parameters : 6815744 || All params : 2723880960 || Trainable% : 681574400\n",
      "Trainable parameters : 6815744 || All params : 2746425344 || Trainable% : 681574400\n",
      "Trainable parameters : 6815744 || All params : 2768969728 || Trainable% : 681574400\n",
      "Trainable parameters : 6815744 || All params : 2768973824 || Trainable% : 681574400\n",
      "Trainable parameters : 6815744 || All params : 2768977920 || Trainable% : 681574400\n",
      "Trainable parameters : 6815744 || All params : 2777366528 || Trainable% : 681574400\n",
      "Trainable parameters : 6848512 || All params : 2777399296 || Trainable% : 684851200\n",
      "Trainable parameters : 6881280 || All params : 2777432064 || Trainable% : 688128000\n",
      "Trainable parameters : 6881280 || All params : 2785820672 || Trainable% : 688128000\n",
      "Trainable parameters : 6914048 || All params : 2785853440 || Trainable% : 691404800\n",
      "Trainable parameters : 6946816 || All params : 2785886208 || Trainable% : 694681600\n",
      "Trainable parameters : 6946816 || All params : 2794274816 || Trainable% : 694681600\n",
      "Trainable parameters : 6979584 || All params : 2794307584 || Trainable% : 697958400\n",
      "Trainable parameters : 7012352 || All params : 2794340352 || Trainable% : 701235200\n",
      "Trainable parameters : 7012352 || All params : 2802728960 || Trainable% : 701235200\n",
      "Trainable parameters : 7045120 || All params : 2802761728 || Trainable% : 704512000\n",
      "Trainable parameters : 7077888 || All params : 2802794496 || Trainable% : 707788800\n",
      "Trainable parameters : 7077888 || All params : 2825338880 || Trainable% : 707788800\n",
      "Trainable parameters : 7077888 || All params : 2847883264 || Trainable% : 707788800\n",
      "Trainable parameters : 7077888 || All params : 2870427648 || Trainable% : 707788800\n",
      "Trainable parameters : 7077888 || All params : 2870431744 || Trainable% : 707788800\n",
      "Trainable parameters : 7077888 || All params : 2870435840 || Trainable% : 707788800\n",
      "Trainable parameters : 7077888 || All params : 2878824448 || Trainable% : 707788800\n",
      "Trainable parameters : 7110656 || All params : 2878857216 || Trainable% : 711065600\n",
      "Trainable parameters : 7143424 || All params : 2878889984 || Trainable% : 714342400\n",
      "Trainable parameters : 7143424 || All params : 2887278592 || Trainable% : 714342400\n",
      "Trainable parameters : 7176192 || All params : 2887311360 || Trainable% : 717619200\n",
      "Trainable parameters : 7208960 || All params : 2887344128 || Trainable% : 720896000\n",
      "Trainable parameters : 7208960 || All params : 2895732736 || Trainable% : 720896000\n",
      "Trainable parameters : 7241728 || All params : 2895765504 || Trainable% : 724172800\n",
      "Trainable parameters : 7274496 || All params : 2895798272 || Trainable% : 727449600\n",
      "Trainable parameters : 7274496 || All params : 2904186880 || Trainable% : 727449600\n",
      "Trainable parameters : 7307264 || All params : 2904219648 || Trainable% : 730726400\n",
      "Trainable parameters : 7340032 || All params : 2904252416 || Trainable% : 734003200\n",
      "Trainable parameters : 7340032 || All params : 2926796800 || Trainable% : 734003200\n",
      "Trainable parameters : 7340032 || All params : 2949341184 || Trainable% : 734003200\n",
      "Trainable parameters : 7340032 || All params : 2971885568 || Trainable% : 734003200\n",
      "Trainable parameters : 7340032 || All params : 2971889664 || Trainable% : 734003200\n",
      "Trainable parameters : 7340032 || All params : 2971893760 || Trainable% : 734003200\n",
      "Trainable parameters : 7340032 || All params : 2980282368 || Trainable% : 734003200\n",
      "Trainable parameters : 7372800 || All params : 2980315136 || Trainable% : 737280000\n",
      "Trainable parameters : 7405568 || All params : 2980347904 || Trainable% : 740556800\n",
      "Trainable parameters : 7405568 || All params : 2988736512 || Trainable% : 740556800\n",
      "Trainable parameters : 7438336 || All params : 2988769280 || Trainable% : 743833600\n",
      "Trainable parameters : 7471104 || All params : 2988802048 || Trainable% : 747110400\n",
      "Trainable parameters : 7471104 || All params : 2997190656 || Trainable% : 747110400\n",
      "Trainable parameters : 7503872 || All params : 2997223424 || Trainable% : 750387200\n",
      "Trainable parameters : 7536640 || All params : 2997256192 || Trainable% : 753664000\n",
      "Trainable parameters : 7536640 || All params : 3005644800 || Trainable% : 753664000\n",
      "Trainable parameters : 7569408 || All params : 3005677568 || Trainable% : 756940800\n",
      "Trainable parameters : 7602176 || All params : 3005710336 || Trainable% : 760217600\n",
      "Trainable parameters : 7602176 || All params : 3028254720 || Trainable% : 760217600\n",
      "Trainable parameters : 7602176 || All params : 3050799104 || Trainable% : 760217600\n",
      "Trainable parameters : 7602176 || All params : 3073343488 || Trainable% : 760217600\n",
      "Trainable parameters : 7602176 || All params : 3073347584 || Trainable% : 760217600\n",
      "Trainable parameters : 7602176 || All params : 3073351680 || Trainable% : 760217600\n",
      "Trainable parameters : 7602176 || All params : 3081740288 || Trainable% : 760217600\n",
      "Trainable parameters : 7634944 || All params : 3081773056 || Trainable% : 763494400\n",
      "Trainable parameters : 7667712 || All params : 3081805824 || Trainable% : 766771200\n",
      "Trainable parameters : 7667712 || All params : 3090194432 || Trainable% : 766771200\n",
      "Trainable parameters : 7700480 || All params : 3090227200 || Trainable% : 770048000\n",
      "Trainable parameters : 7733248 || All params : 3090259968 || Trainable% : 773324800\n",
      "Trainable parameters : 7733248 || All params : 3098648576 || Trainable% : 773324800\n",
      "Trainable parameters : 7766016 || All params : 3098681344 || Trainable% : 776601600\n",
      "Trainable parameters : 7798784 || All params : 3098714112 || Trainable% : 779878400\n",
      "Trainable parameters : 7798784 || All params : 3107102720 || Trainable% : 779878400\n",
      "Trainable parameters : 7831552 || All params : 3107135488 || Trainable% : 783155200\n",
      "Trainable parameters : 7864320 || All params : 3107168256 || Trainable% : 786432000\n",
      "Trainable parameters : 7864320 || All params : 3129712640 || Trainable% : 786432000\n",
      "Trainable parameters : 7864320 || All params : 3152257024 || Trainable% : 786432000\n",
      "Trainable parameters : 7864320 || All params : 3174801408 || Trainable% : 786432000\n",
      "Trainable parameters : 7864320 || All params : 3174805504 || Trainable% : 786432000\n",
      "Trainable parameters : 7864320 || All params : 3174809600 || Trainable% : 786432000\n",
      "Trainable parameters : 7864320 || All params : 3183198208 || Trainable% : 786432000\n",
      "Trainable parameters : 7897088 || All params : 3183230976 || Trainable% : 789708800\n",
      "Trainable parameters : 7929856 || All params : 3183263744 || Trainable% : 792985600\n",
      "Trainable parameters : 7929856 || All params : 3191652352 || Trainable% : 792985600\n",
      "Trainable parameters : 7962624 || All params : 3191685120 || Trainable% : 796262400\n",
      "Trainable parameters : 7995392 || All params : 3191717888 || Trainable% : 799539200\n",
      "Trainable parameters : 7995392 || All params : 3200106496 || Trainable% : 799539200\n",
      "Trainable parameters : 8028160 || All params : 3200139264 || Trainable% : 802816000\n",
      "Trainable parameters : 8060928 || All params : 3200172032 || Trainable% : 806092800\n",
      "Trainable parameters : 8060928 || All params : 3208560640 || Trainable% : 806092800\n",
      "Trainable parameters : 8093696 || All params : 3208593408 || Trainable% : 809369600\n",
      "Trainable parameters : 8126464 || All params : 3208626176 || Trainable% : 812646400\n",
      "Trainable parameters : 8126464 || All params : 3231170560 || Trainable% : 812646400\n",
      "Trainable parameters : 8126464 || All params : 3253714944 || Trainable% : 812646400\n",
      "Trainable parameters : 8126464 || All params : 3276259328 || Trainable% : 812646400\n",
      "Trainable parameters : 8126464 || All params : 3276263424 || Trainable% : 812646400\n",
      "Trainable parameters : 8126464 || All params : 3276267520 || Trainable% : 812646400\n",
      "Trainable parameters : 8126464 || All params : 3284656128 || Trainable% : 812646400\n",
      "Trainable parameters : 8159232 || All params : 3284688896 || Trainable% : 815923200\n",
      "Trainable parameters : 8192000 || All params : 3284721664 || Trainable% : 819200000\n",
      "Trainable parameters : 8192000 || All params : 3293110272 || Trainable% : 819200000\n",
      "Trainable parameters : 8224768 || All params : 3293143040 || Trainable% : 822476800\n",
      "Trainable parameters : 8257536 || All params : 3293175808 || Trainable% : 825753600\n",
      "Trainable parameters : 8257536 || All params : 3301564416 || Trainable% : 825753600\n",
      "Trainable parameters : 8290304 || All params : 3301597184 || Trainable% : 829030400\n",
      "Trainable parameters : 8323072 || All params : 3301629952 || Trainable% : 832307200\n",
      "Trainable parameters : 8323072 || All params : 3310018560 || Trainable% : 832307200\n",
      "Trainable parameters : 8355840 || All params : 3310051328 || Trainable% : 835584000\n",
      "Trainable parameters : 8388608 || All params : 3310084096 || Trainable% : 838860800\n",
      "Trainable parameters : 8388608 || All params : 3332628480 || Trainable% : 838860800\n",
      "Trainable parameters : 8388608 || All params : 3355172864 || Trainable% : 838860800\n",
      "Trainable parameters : 8388608 || All params : 3377717248 || Trainable% : 838860800\n",
      "Trainable parameters : 8388608 || All params : 3377721344 || Trainable% : 838860800\n",
      "Trainable parameters : 8388608 || All params : 3377725440 || Trainable% : 838860800\n",
      "Trainable parameters : 8388608 || All params : 3377729536 || Trainable% : 838860800\n",
      "Trainable parameters : 8388608 || All params : 3508801536 || Trainable% : 838860800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 32,\n",
    "    target_modules = [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\"],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = 'none',\n",
    "    task_type = 'CAUSAL_LM'\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea0bf8ab-7b92-4ac4-85f4-63eb06b60df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|████████████████████████████████████████████| 2508/2508 [00:00<00:00, 114537.08 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 2508/2508 [00:00<00:00, 23428.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples['quote']), batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f28e0-b47e-4bbb-a5f5-5e44f6829577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "453b398a-e201-48b1-8c13-b7a4a227daa8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47a3c568-e050-4321-b653-f64dd406ec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/root/anaconda3/envs/tf/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.658600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.963700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.996500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.900400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.159000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=1.932924735546112, metrics={'train_runtime': 51.231, 'train_samples_per_second': 0.781, 'train_steps_per_second': 0.195, 'total_flos': 60375172276224.0, 'train_loss': 1.932924735546112, 'epoch': 0.01594896331738437})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token #</s> for llama\n",
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    train_dataset=data['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir = \"outputs\",\n",
    "        optim = \"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05408a0d-af5c-40af-88f6-328d9408ac09",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2b4740a-4e16-4e26-b3b3-900907fc83eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModelForCausalLM(\n",
       "      (base_model): LoraModel(\n",
       "        (model): PeftModelForCausalLM(\n",
       "          (base_model): LoraModel(\n",
       "            (model): PeftModelForCausalLM(\n",
       "              (base_model): LoraModel(\n",
       "                (model): LlamaForCausalLM(\n",
       "                  (model): LlamaModel(\n",
       "                    (embed_tokens): Embedding(32000, 4096)\n",
       "                    (layers): ModuleList(\n",
       "                      (0-31): 32 x LlamaDecoderLayer(\n",
       "                        (self_attn): LlamaAttention(\n",
       "                          (q_proj): lora.Linear4bit(\n",
       "                            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                            (lora_dropout): ModuleDict(\n",
       "                              (default): Dropout(p=0.05, inplace=False)\n",
       "                            )\n",
       "                            (lora_A): ModuleDict(\n",
       "                              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                            )\n",
       "                            (lora_B): ModuleDict(\n",
       "                              (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                            )\n",
       "                            (lora_embedding_A): ParameterDict()\n",
       "                            (lora_embedding_B): ParameterDict()\n",
       "                            (lora_magnitude_vector): ModuleDict()\n",
       "                          )\n",
       "                          (k_proj): lora.Linear4bit(\n",
       "                            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                            (lora_dropout): ModuleDict(\n",
       "                              (default): Dropout(p=0.05, inplace=False)\n",
       "                            )\n",
       "                            (lora_A): ModuleDict(\n",
       "                              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                            )\n",
       "                            (lora_B): ModuleDict(\n",
       "                              (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                            )\n",
       "                            (lora_embedding_A): ParameterDict()\n",
       "                            (lora_embedding_B): ParameterDict()\n",
       "                            (lora_magnitude_vector): ModuleDict()\n",
       "                          )\n",
       "                          (v_proj): lora.Linear4bit(\n",
       "                            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                            (lora_dropout): ModuleDict(\n",
       "                              (default): Dropout(p=0.05, inplace=False)\n",
       "                            )\n",
       "                            (lora_A): ModuleDict(\n",
       "                              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                            )\n",
       "                            (lora_B): ModuleDict(\n",
       "                              (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                            )\n",
       "                            (lora_embedding_A): ParameterDict()\n",
       "                            (lora_embedding_B): ParameterDict()\n",
       "                            (lora_magnitude_vector): ModuleDict()\n",
       "                          )\n",
       "                          (o_proj): lora.Linear4bit(\n",
       "                            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                            (lora_dropout): ModuleDict(\n",
       "                              (default): Dropout(p=0.05, inplace=False)\n",
       "                            )\n",
       "                            (lora_A): ModuleDict(\n",
       "                              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                            )\n",
       "                            (lora_B): ModuleDict(\n",
       "                              (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                            )\n",
       "                            (lora_embedding_A): ParameterDict()\n",
       "                            (lora_embedding_B): ParameterDict()\n",
       "                            (lora_magnitude_vector): ModuleDict()\n",
       "                          )\n",
       "                        )\n",
       "                        (mlp): LlamaMLP(\n",
       "                          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "                          (act_fn): SiLU()\n",
       "                        )\n",
       "                        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "                        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "                      )\n",
       "                    )\n",
       "                    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "                    (rotary_emb): LlamaRotaryEmbedding()\n",
       "                  )\n",
       "                  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8cdff268-31bb-4bda-9403-284afab32684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    system_prompt = 'You are a helpful assistant that provides accurate and concise responses'\n",
    "\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "    prompt = f\"{B_INST} {B_SYS} {system_prompt.strip()}{E_SYS}{user_prompt.strip()}{E_INST}\\n\\n\"\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    _ = model.generate(**inputs, streamer = streamer, max_new_tokens = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e06dd28d-88a9-4b8d-9d08-c534ac45a97b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] <<SYS>>\n",
      " You are a helpful assistant that provides accurate and concise responses\n",
      "<</SYS>>\n",
      "\n",
      "provide a brief comparison of salsa and bachata[/INST]\n",
      "\n",
      "Sure, I'd be happy to help! Here's a brief comparison of salsa and bachata:\n",
      "\n",
      "Salsa and bachata are both popular Latin dance styles that originated in Latin America. However, they have some key differences:\n",
      "\n",
      "1. Origin: Salsa originated in Cuba and Puerto Rico in the 1940s and 1950s, while bachata originated in the Dominican Republic in the 1960s.\n",
      "2. Rhythm: Salsa has a faster, more energetic rhythm than bachata, with a tempo of around 160-180 beats per minute (BPM). Bachata has a slower, more romantic rhythm, with a tempo of around 120-140 BPM.\n",
      "3. Steps: Salsa involves a variety of quick footwork patterns, including forward and backward walks, side steps, and turns. Bachata features a more relaxed, flowing style of footwork, with a focus on hip movement and turns.\n",
      "4. Music: Salsa music is typically more upbeat and energetic, with a focus on horns, percussion, and fast-paced rhythms. Bachata music is more mellow and romantic, with a focus on melodic instruments like the guitar and bass.\n",
      "5. Social scene: Salsa is a popular social dance style, often danced at nightclubs and parties. Bachata is also a social dance, but it is more commonly danced at ballrooms and social events.\n",
      "\n",
      "Overall, while both salsa and bachata are fun and energetic dance styles, they have distinct differences in origin, rhythm, steps, music, and social scene.</s>\n"
     ]
    }
   ],
   "source": [
    "stream('provide a brief comparison of salsa and bachata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0aec47-fcea-4afa-8221-871c9e58cbdd",
   "metadata": {},
   "source": [
    "# Advanced FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da213b11-7171-4b7b-8fff-5b26bb2f9ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb6fd21-727d-4943-9ec5-f07b12676e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(os.getcwd(), \"Model_cache_directory\")\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cded0e14-9f36-440f-a715-9605e8cd6985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c53831a5d14f01a285f3ec1fd77be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model, \n",
    "    quantization_config = bnb_config, \n",
    "    device_map = {\"\":0},\n",
    "    trust_remote_code = True,\n",
    "    cache_dir = cache_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7664c9bf-8435-4120-b33d-7bdf46ef777e",
   "metadata": {},
   "source": [
    "# Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03fccf73-bc41-40f2-82a4-fa4bb5f611e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, cache_dir = cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada3f454-d9a4-4e89-9f29-43f35b805c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS Token :  </s>\n",
      "EOS Token ID :  2\n"
     ]
    }
   ],
   "source": [
    "print(\"EOS Token : \",tokenizer.eos_token)\n",
    "print(\"EOS Token ID : \",tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1913f809-57f8-4084-a2d0-2d6d45f8c4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Token :  None\n",
      "Pad Token ID :  None\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad Token : \",tokenizer.pad_token)\n",
    "print(\"Pad Token ID : \",tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64700eea-66bd-4e8b-9550-e70b594253a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e62d0-8d4f-475f-9dd4-03ee7abe97dd",
   "metadata": {},
   "source": [
    "## Setup Pad token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4f5d401-de2b-4efc-afae-8cad42a94820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer pad token ID :  32000\n",
      "Tokenizer pad token ID :  32000\n",
      "Model pad token ID :  32000\n",
      "Model Config pad token ID :  32000\n",
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if '|<pad>|' not in tokenizer.get_vocab():\n",
    "    tokenizer.add_tokens(['|<pad>|'])\n",
    "tokenizer.pad_token = '|<pad>|'\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "assert model.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizers pad token id\"\n",
    "\n",
    "print(\"Tokenizer pad token ID : \", tokenizer.pad_token_id)\n",
    "\n",
    "print('Tokenizer pad token ID : ',tokenizer.pad_token_id)\n",
    "print('Model pad token ID : ',model.pad_token_id)\n",
    "print('Model Config pad token ID : ',model.config.pad_token_id)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d81d210-9c4e-4618-95e9-443873cd3f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of the sequence : <s>[INST] (BOS token :<s>, id:1)\n",
      "End of the sequence : <s>[INST] (BOS token :</s>, id:2)\n",
      "The number of tokens in the string is : 2\n",
      "The ids are : {'input_ids': [[1, 1, 518, 25580, 29962]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n",
      "THe decoded string is : <s><s> [INST]\n"
     ]
    }
   ],
   "source": [
    "sample_string = ['<s>[INST]']\n",
    "\n",
    "#Tokenize the stringified JSON Object\n",
    "encoded_sample = tokenizer(sample_string, truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "#Count the number of tokens\n",
    "token_count = len(encoded_sample)\n",
    "\n",
    "#Fetch BOS and EOS tokens\n",
    "BOS_token_id = tokenizer.bos_token_id\n",
    "EOS_token_id = tokenizer.eos_token_id\n",
    "BOS_token = tokenizer.decode([BOS_token_id])\n",
    "EOS_token = tokenizer.decode([EOS_token_id])\n",
    "\n",
    "#Check and print BOS and EOS tokens\n",
    "print(f\"Beginning of the sequence : {sample_string[0]} (BOS token :{BOS_token}, id:{BOS_token_id})\")\n",
    "print(f\"End of the sequence : {sample_string[-1]} (BOS token :{EOS_token}, id:{EOS_token_id})\")\n",
    "\n",
    "print(f\"The number of tokens in the string is : {token_count}\")\n",
    "print(f\"The ids are : {encoded_sample}\")\n",
    "\n",
    "#Decode the input_ids\n",
    "decoded_sample = tokenizer.decode(encoded_sample['input_ids'][0], skip_special_tokens=False)\n",
    "\n",
    "#Print decoded Strings\n",
    "print(f\"THe decoded string is : {decoded_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1235db1-b137-4e3d-9690-40337b67419d",
   "metadata": {},
   "source": [
    "# Setup Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a37c51ac-bc10-4621-b84d-c61b8c22dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e2cd8d-0eea-4289-99c5-8468f2fc6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _,param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "\n",
    "    print(f\"Trainable params : {trainable_params} || All params : {all_param} || Trainable % : {100 * trainable_params}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c630d87-0e31-4a4d-92d1-7c84da46ddc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.weight.absmax', 'model.layers.0.self_attn.q_proj.weight.quant_map', 'model.layers.0.self_attn.q_proj.weight.nested_absmax', 'model.layers.0.self_attn.q_proj.weight.nested_quant_map', 'model.layers.0.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.weight.absmax', 'model.layers.0.self_attn.k_proj.weight.quant_map', 'model.layers.0.self_attn.k_proj.weight.nested_absmax', 'model.layers.0.self_attn.k_proj.weight.nested_quant_map', 'model.layers.0.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.weight.absmax', 'model.layers.0.self_attn.v_proj.weight.quant_map', 'model.layers.0.self_attn.v_proj.weight.nested_absmax', 'model.layers.0.self_attn.v_proj.weight.nested_quant_map', 'model.layers.0.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.weight.absmax', 'model.layers.0.self_attn.o_proj.weight.quant_map', 'model.layers.0.self_attn.o_proj.weight.nested_absmax', 'model.layers.0.self_attn.o_proj.weight.nested_quant_map', 'model.layers.0.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.gate_proj.weight.absmax', 'model.layers.0.mlp.gate_proj.weight.quant_map', 'model.layers.0.mlp.gate_proj.weight.nested_absmax', 'model.layers.0.mlp.gate_proj.weight.nested_quant_map', 'model.layers.0.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.up_proj.weight.absmax', 'model.layers.0.mlp.up_proj.weight.quant_map', 'model.layers.0.mlp.up_proj.weight.nested_absmax', 'model.layers.0.mlp.up_proj.weight.nested_quant_map', 'model.layers.0.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.down_proj.weight.absmax', 'model.layers.0.mlp.down_proj.weight.quant_map', 'model.layers.0.mlp.down_proj.weight.nested_absmax', 'model.layers.0.mlp.down_proj.weight.nested_quant_map', 'model.layers.0.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight.absmax', 'model.layers.1.self_attn.q_proj.weight.quant_map', 'model.layers.1.self_attn.q_proj.weight.nested_absmax', 'model.layers.1.self_attn.q_proj.weight.nested_quant_map', 'model.layers.1.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.weight.absmax', 'model.layers.1.self_attn.k_proj.weight.quant_map', 'model.layers.1.self_attn.k_proj.weight.nested_absmax', 'model.layers.1.self_attn.k_proj.weight.nested_quant_map', 'model.layers.1.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.weight.absmax', 'model.layers.1.self_attn.v_proj.weight.quant_map', 'model.layers.1.self_attn.v_proj.weight.nested_absmax', 'model.layers.1.self_attn.v_proj.weight.nested_quant_map', 'model.layers.1.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.o_proj.weight.absmax', 'model.layers.1.self_attn.o_proj.weight.quant_map', 'model.layers.1.self_attn.o_proj.weight.nested_absmax', 'model.layers.1.self_attn.o_proj.weight.nested_quant_map', 'model.layers.1.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.gate_proj.weight.absmax', 'model.layers.1.mlp.gate_proj.weight.quant_map', 'model.layers.1.mlp.gate_proj.weight.nested_absmax', 'model.layers.1.mlp.gate_proj.weight.nested_quant_map', 'model.layers.1.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.up_proj.weight.absmax', 'model.layers.1.mlp.up_proj.weight.quant_map', 'model.layers.1.mlp.up_proj.weight.nested_absmax', 'model.layers.1.mlp.up_proj.weight.nested_quant_map', 'model.layers.1.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.down_proj.weight.absmax', 'model.layers.1.mlp.down_proj.weight.quant_map', 'model.layers.1.mlp.down_proj.weight.nested_absmax', 'model.layers.1.mlp.down_proj.weight.nested_quant_map', 'model.layers.1.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.weight.absmax', 'model.layers.2.self_attn.q_proj.weight.quant_map', 'model.layers.2.self_attn.q_proj.weight.nested_absmax', 'model.layers.2.self_attn.q_proj.weight.nested_quant_map', 'model.layers.2.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.weight.absmax', 'model.layers.2.self_attn.k_proj.weight.quant_map', 'model.layers.2.self_attn.k_proj.weight.nested_absmax', 'model.layers.2.self_attn.k_proj.weight.nested_quant_map', 'model.layers.2.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.v_proj.weight.absmax', 'model.layers.2.self_attn.v_proj.weight.quant_map', 'model.layers.2.self_attn.v_proj.weight.nested_absmax', 'model.layers.2.self_attn.v_proj.weight.nested_quant_map', 'model.layers.2.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.o_proj.weight.absmax', 'model.layers.2.self_attn.o_proj.weight.quant_map', 'model.layers.2.self_attn.o_proj.weight.nested_absmax', 'model.layers.2.self_attn.o_proj.weight.nested_quant_map', 'model.layers.2.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.gate_proj.weight.absmax', 'model.layers.2.mlp.gate_proj.weight.quant_map', 'model.layers.2.mlp.gate_proj.weight.nested_absmax', 'model.layers.2.mlp.gate_proj.weight.nested_quant_map', 'model.layers.2.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.up_proj.weight.absmax', 'model.layers.2.mlp.up_proj.weight.quant_map', 'model.layers.2.mlp.up_proj.weight.nested_absmax', 'model.layers.2.mlp.up_proj.weight.nested_quant_map', 'model.layers.2.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.down_proj.weight.absmax', 'model.layers.2.mlp.down_proj.weight.quant_map', 'model.layers.2.mlp.down_proj.weight.nested_absmax', 'model.layers.2.mlp.down_proj.weight.nested_quant_map', 'model.layers.2.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.weight.absmax', 'model.layers.3.self_attn.q_proj.weight.quant_map', 'model.layers.3.self_attn.q_proj.weight.nested_absmax', 'model.layers.3.self_attn.q_proj.weight.nested_quant_map', 'model.layers.3.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.weight.absmax', 'model.layers.3.self_attn.k_proj.weight.quant_map', 'model.layers.3.self_attn.k_proj.weight.nested_absmax', 'model.layers.3.self_attn.k_proj.weight.nested_quant_map', 'model.layers.3.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight.absmax', 'model.layers.3.self_attn.v_proj.weight.quant_map', 'model.layers.3.self_attn.v_proj.weight.nested_absmax', 'model.layers.3.self_attn.v_proj.weight.nested_quant_map', 'model.layers.3.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.o_proj.weight.absmax', 'model.layers.3.self_attn.o_proj.weight.quant_map', 'model.layers.3.self_attn.o_proj.weight.nested_absmax', 'model.layers.3.self_attn.o_proj.weight.nested_quant_map', 'model.layers.3.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.gate_proj.weight.absmax', 'model.layers.3.mlp.gate_proj.weight.quant_map', 'model.layers.3.mlp.gate_proj.weight.nested_absmax', 'model.layers.3.mlp.gate_proj.weight.nested_quant_map', 'model.layers.3.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.up_proj.weight.absmax', 'model.layers.3.mlp.up_proj.weight.quant_map', 'model.layers.3.mlp.up_proj.weight.nested_absmax', 'model.layers.3.mlp.up_proj.weight.nested_quant_map', 'model.layers.3.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.down_proj.weight.absmax', 'model.layers.3.mlp.down_proj.weight.quant_map', 'model.layers.3.mlp.down_proj.weight.nested_absmax', 'model.layers.3.mlp.down_proj.weight.nested_quant_map', 'model.layers.3.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.weight.absmax', 'model.layers.4.self_attn.q_proj.weight.quant_map', 'model.layers.4.self_attn.q_proj.weight.nested_absmax', 'model.layers.4.self_attn.q_proj.weight.nested_quant_map', 'model.layers.4.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.weight.absmax', 'model.layers.4.self_attn.k_proj.weight.quant_map', 'model.layers.4.self_attn.k_proj.weight.nested_absmax', 'model.layers.4.self_attn.k_proj.weight.nested_quant_map', 'model.layers.4.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.v_proj.weight.absmax', 'model.layers.4.self_attn.v_proj.weight.quant_map', 'model.layers.4.self_attn.v_proj.weight.nested_absmax', 'model.layers.4.self_attn.v_proj.weight.nested_quant_map', 'model.layers.4.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.weight.absmax', 'model.layers.4.self_attn.o_proj.weight.quant_map', 'model.layers.4.self_attn.o_proj.weight.nested_absmax', 'model.layers.4.self_attn.o_proj.weight.nested_quant_map', 'model.layers.4.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.gate_proj.weight.absmax', 'model.layers.4.mlp.gate_proj.weight.quant_map', 'model.layers.4.mlp.gate_proj.weight.nested_absmax', 'model.layers.4.mlp.gate_proj.weight.nested_quant_map', 'model.layers.4.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.up_proj.weight.absmax', 'model.layers.4.mlp.up_proj.weight.quant_map', 'model.layers.4.mlp.up_proj.weight.nested_absmax', 'model.layers.4.mlp.up_proj.weight.nested_quant_map', 'model.layers.4.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.down_proj.weight.absmax', 'model.layers.4.mlp.down_proj.weight.quant_map', 'model.layers.4.mlp.down_proj.weight.nested_absmax', 'model.layers.4.mlp.down_proj.weight.nested_quant_map', 'model.layers.4.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.q_proj.weight.absmax', 'model.layers.5.self_attn.q_proj.weight.quant_map', 'model.layers.5.self_attn.q_proj.weight.nested_absmax', 'model.layers.5.self_attn.q_proj.weight.nested_quant_map', 'model.layers.5.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.weight.absmax', 'model.layers.5.self_attn.k_proj.weight.quant_map', 'model.layers.5.self_attn.k_proj.weight.nested_absmax', 'model.layers.5.self_attn.k_proj.weight.nested_quant_map', 'model.layers.5.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.weight.absmax', 'model.layers.5.self_attn.v_proj.weight.quant_map', 'model.layers.5.self_attn.v_proj.weight.nested_absmax', 'model.layers.5.self_attn.v_proj.weight.nested_quant_map', 'model.layers.5.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.o_proj.weight.absmax', 'model.layers.5.self_attn.o_proj.weight.quant_map', 'model.layers.5.self_attn.o_proj.weight.nested_absmax', 'model.layers.5.self_attn.o_proj.weight.nested_quant_map', 'model.layers.5.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.gate_proj.weight.absmax', 'model.layers.5.mlp.gate_proj.weight.quant_map', 'model.layers.5.mlp.gate_proj.weight.nested_absmax', 'model.layers.5.mlp.gate_proj.weight.nested_quant_map', 'model.layers.5.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.up_proj.weight.absmax', 'model.layers.5.mlp.up_proj.weight.quant_map', 'model.layers.5.mlp.up_proj.weight.nested_absmax', 'model.layers.5.mlp.up_proj.weight.nested_quant_map', 'model.layers.5.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight.absmax', 'model.layers.5.mlp.down_proj.weight.quant_map', 'model.layers.5.mlp.down_proj.weight.nested_absmax', 'model.layers.5.mlp.down_proj.weight.nested_quant_map', 'model.layers.5.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.weight.absmax', 'model.layers.6.self_attn.q_proj.weight.quant_map', 'model.layers.6.self_attn.q_proj.weight.nested_absmax', 'model.layers.6.self_attn.q_proj.weight.nested_quant_map', 'model.layers.6.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.weight.absmax', 'model.layers.6.self_attn.k_proj.weight.quant_map', 'model.layers.6.self_attn.k_proj.weight.nested_absmax', 'model.layers.6.self_attn.k_proj.weight.nested_quant_map', 'model.layers.6.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight.absmax', 'model.layers.6.self_attn.v_proj.weight.quant_map', 'model.layers.6.self_attn.v_proj.weight.nested_absmax', 'model.layers.6.self_attn.v_proj.weight.nested_quant_map', 'model.layers.6.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.o_proj.weight.absmax', 'model.layers.6.self_attn.o_proj.weight.quant_map', 'model.layers.6.self_attn.o_proj.weight.nested_absmax', 'model.layers.6.self_attn.o_proj.weight.nested_quant_map', 'model.layers.6.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.gate_proj.weight.absmax', 'model.layers.6.mlp.gate_proj.weight.quant_map', 'model.layers.6.mlp.gate_proj.weight.nested_absmax', 'model.layers.6.mlp.gate_proj.weight.nested_quant_map', 'model.layers.6.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.up_proj.weight.absmax', 'model.layers.6.mlp.up_proj.weight.quant_map', 'model.layers.6.mlp.up_proj.weight.nested_absmax', 'model.layers.6.mlp.up_proj.weight.nested_quant_map', 'model.layers.6.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight.absmax', 'model.layers.6.mlp.down_proj.weight.quant_map', 'model.layers.6.mlp.down_proj.weight.nested_absmax', 'model.layers.6.mlp.down_proj.weight.nested_quant_map', 'model.layers.6.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight.absmax', 'model.layers.7.self_attn.q_proj.weight.quant_map', 'model.layers.7.self_attn.q_proj.weight.nested_absmax', 'model.layers.7.self_attn.q_proj.weight.nested_quant_map', 'model.layers.7.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.weight.absmax', 'model.layers.7.self_attn.k_proj.weight.quant_map', 'model.layers.7.self_attn.k_proj.weight.nested_absmax', 'model.layers.7.self_attn.k_proj.weight.nested_quant_map', 'model.layers.7.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.weight.absmax', 'model.layers.7.self_attn.v_proj.weight.quant_map', 'model.layers.7.self_attn.v_proj.weight.nested_absmax', 'model.layers.7.self_attn.v_proj.weight.nested_quant_map', 'model.layers.7.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.o_proj.weight.absmax', 'model.layers.7.self_attn.o_proj.weight.quant_map', 'model.layers.7.self_attn.o_proj.weight.nested_absmax', 'model.layers.7.self_attn.o_proj.weight.nested_quant_map', 'model.layers.7.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.gate_proj.weight.absmax', 'model.layers.7.mlp.gate_proj.weight.quant_map', 'model.layers.7.mlp.gate_proj.weight.nested_absmax', 'model.layers.7.mlp.gate_proj.weight.nested_quant_map', 'model.layers.7.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.up_proj.weight.absmax', 'model.layers.7.mlp.up_proj.weight.quant_map', 'model.layers.7.mlp.up_proj.weight.nested_absmax', 'model.layers.7.mlp.up_proj.weight.nested_quant_map', 'model.layers.7.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight.absmax', 'model.layers.7.mlp.down_proj.weight.quant_map', 'model.layers.7.mlp.down_proj.weight.nested_absmax', 'model.layers.7.mlp.down_proj.weight.nested_quant_map', 'model.layers.7.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.weight.absmax', 'model.layers.8.self_attn.q_proj.weight.quant_map', 'model.layers.8.self_attn.q_proj.weight.nested_absmax', 'model.layers.8.self_attn.q_proj.weight.nested_quant_map', 'model.layers.8.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight.absmax', 'model.layers.8.self_attn.k_proj.weight.quant_map', 'model.layers.8.self_attn.k_proj.weight.nested_absmax', 'model.layers.8.self_attn.k_proj.weight.nested_quant_map', 'model.layers.8.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.weight.absmax', 'model.layers.8.self_attn.v_proj.weight.quant_map', 'model.layers.8.self_attn.v_proj.weight.nested_absmax', 'model.layers.8.self_attn.v_proj.weight.nested_quant_map', 'model.layers.8.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.o_proj.weight.absmax', 'model.layers.8.self_attn.o_proj.weight.quant_map', 'model.layers.8.self_attn.o_proj.weight.nested_absmax', 'model.layers.8.self_attn.o_proj.weight.nested_quant_map', 'model.layers.8.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.gate_proj.weight.absmax', 'model.layers.8.mlp.gate_proj.weight.quant_map', 'model.layers.8.mlp.gate_proj.weight.nested_absmax', 'model.layers.8.mlp.gate_proj.weight.nested_quant_map', 'model.layers.8.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.up_proj.weight.absmax', 'model.layers.8.mlp.up_proj.weight.quant_map', 'model.layers.8.mlp.up_proj.weight.nested_absmax', 'model.layers.8.mlp.up_proj.weight.nested_quant_map', 'model.layers.8.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight.absmax', 'model.layers.8.mlp.down_proj.weight.quant_map', 'model.layers.8.mlp.down_proj.weight.nested_absmax', 'model.layers.8.mlp.down_proj.weight.nested_quant_map', 'model.layers.8.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.q_proj.weight.absmax', 'model.layers.9.self_attn.q_proj.weight.quant_map', 'model.layers.9.self_attn.q_proj.weight.nested_absmax', 'model.layers.9.self_attn.q_proj.weight.nested_quant_map', 'model.layers.9.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.weight.absmax', 'model.layers.9.self_attn.k_proj.weight.quant_map', 'model.layers.9.self_attn.k_proj.weight.nested_absmax', 'model.layers.9.self_attn.k_proj.weight.nested_quant_map', 'model.layers.9.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.weight.absmax', 'model.layers.9.self_attn.v_proj.weight.quant_map', 'model.layers.9.self_attn.v_proj.weight.nested_absmax', 'model.layers.9.self_attn.v_proj.weight.nested_quant_map', 'model.layers.9.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.o_proj.weight.absmax', 'model.layers.9.self_attn.o_proj.weight.quant_map', 'model.layers.9.self_attn.o_proj.weight.nested_absmax', 'model.layers.9.self_attn.o_proj.weight.nested_quant_map', 'model.layers.9.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.gate_proj.weight.absmax', 'model.layers.9.mlp.gate_proj.weight.quant_map', 'model.layers.9.mlp.gate_proj.weight.nested_absmax', 'model.layers.9.mlp.gate_proj.weight.nested_quant_map', 'model.layers.9.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.up_proj.weight.absmax', 'model.layers.9.mlp.up_proj.weight.quant_map', 'model.layers.9.mlp.up_proj.weight.nested_absmax', 'model.layers.9.mlp.up_proj.weight.nested_quant_map', 'model.layers.9.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.down_proj.weight.absmax', 'model.layers.9.mlp.down_proj.weight.quant_map', 'model.layers.9.mlp.down_proj.weight.nested_absmax', 'model.layers.9.mlp.down_proj.weight.nested_quant_map', 'model.layers.9.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.weight.absmax', 'model.layers.10.self_attn.q_proj.weight.quant_map', 'model.layers.10.self_attn.q_proj.weight.nested_absmax', 'model.layers.10.self_attn.q_proj.weight.nested_quant_map', 'model.layers.10.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.weight.absmax', 'model.layers.10.self_attn.k_proj.weight.quant_map', 'model.layers.10.self_attn.k_proj.weight.nested_absmax', 'model.layers.10.self_attn.k_proj.weight.nested_quant_map', 'model.layers.10.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.weight.absmax', 'model.layers.10.self_attn.v_proj.weight.quant_map', 'model.layers.10.self_attn.v_proj.weight.nested_absmax', 'model.layers.10.self_attn.v_proj.weight.nested_quant_map', 'model.layers.10.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.o_proj.weight.absmax', 'model.layers.10.self_attn.o_proj.weight.quant_map', 'model.layers.10.self_attn.o_proj.weight.nested_absmax', 'model.layers.10.self_attn.o_proj.weight.nested_quant_map', 'model.layers.10.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.gate_proj.weight.absmax', 'model.layers.10.mlp.gate_proj.weight.quant_map', 'model.layers.10.mlp.gate_proj.weight.nested_absmax', 'model.layers.10.mlp.gate_proj.weight.nested_quant_map', 'model.layers.10.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.up_proj.weight.absmax', 'model.layers.10.mlp.up_proj.weight.quant_map', 'model.layers.10.mlp.up_proj.weight.nested_absmax', 'model.layers.10.mlp.up_proj.weight.nested_quant_map', 'model.layers.10.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight.absmax', 'model.layers.10.mlp.down_proj.weight.quant_map', 'model.layers.10.mlp.down_proj.weight.nested_absmax', 'model.layers.10.mlp.down_proj.weight.nested_quant_map', 'model.layers.10.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight.absmax', 'model.layers.11.self_attn.q_proj.weight.quant_map', 'model.layers.11.self_attn.q_proj.weight.nested_absmax', 'model.layers.11.self_attn.q_proj.weight.nested_quant_map', 'model.layers.11.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight.absmax', 'model.layers.11.self_attn.k_proj.weight.quant_map', 'model.layers.11.self_attn.k_proj.weight.nested_absmax', 'model.layers.11.self_attn.k_proj.weight.nested_quant_map', 'model.layers.11.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.weight.absmax', 'model.layers.11.self_attn.v_proj.weight.quant_map', 'model.layers.11.self_attn.v_proj.weight.nested_absmax', 'model.layers.11.self_attn.v_proj.weight.nested_quant_map', 'model.layers.11.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.o_proj.weight.absmax', 'model.layers.11.self_attn.o_proj.weight.quant_map', 'model.layers.11.self_attn.o_proj.weight.nested_absmax', 'model.layers.11.self_attn.o_proj.weight.nested_quant_map', 'model.layers.11.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.gate_proj.weight.absmax', 'model.layers.11.mlp.gate_proj.weight.quant_map', 'model.layers.11.mlp.gate_proj.weight.nested_absmax', 'model.layers.11.mlp.gate_proj.weight.nested_quant_map', 'model.layers.11.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.up_proj.weight.absmax', 'model.layers.11.mlp.up_proj.weight.quant_map', 'model.layers.11.mlp.up_proj.weight.nested_absmax', 'model.layers.11.mlp.up_proj.weight.nested_quant_map', 'model.layers.11.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.down_proj.weight.absmax', 'model.layers.11.mlp.down_proj.weight.quant_map', 'model.layers.11.mlp.down_proj.weight.nested_absmax', 'model.layers.11.mlp.down_proj.weight.nested_quant_map', 'model.layers.11.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.weight.absmax', 'model.layers.12.self_attn.q_proj.weight.quant_map', 'model.layers.12.self_attn.q_proj.weight.nested_absmax', 'model.layers.12.self_attn.q_proj.weight.nested_quant_map', 'model.layers.12.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight.absmax', 'model.layers.12.self_attn.k_proj.weight.quant_map', 'model.layers.12.self_attn.k_proj.weight.nested_absmax', 'model.layers.12.self_attn.k_proj.weight.nested_quant_map', 'model.layers.12.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.v_proj.weight.absmax', 'model.layers.12.self_attn.v_proj.weight.quant_map', 'model.layers.12.self_attn.v_proj.weight.nested_absmax', 'model.layers.12.self_attn.v_proj.weight.nested_quant_map', 'model.layers.12.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.o_proj.weight.absmax', 'model.layers.12.self_attn.o_proj.weight.quant_map', 'model.layers.12.self_attn.o_proj.weight.nested_absmax', 'model.layers.12.self_attn.o_proj.weight.nested_quant_map', 'model.layers.12.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.gate_proj.weight.absmax', 'model.layers.12.mlp.gate_proj.weight.quant_map', 'model.layers.12.mlp.gate_proj.weight.nested_absmax', 'model.layers.12.mlp.gate_proj.weight.nested_quant_map', 'model.layers.12.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.up_proj.weight.absmax', 'model.layers.12.mlp.up_proj.weight.quant_map', 'model.layers.12.mlp.up_proj.weight.nested_absmax', 'model.layers.12.mlp.up_proj.weight.nested_quant_map', 'model.layers.12.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.down_proj.weight.absmax', 'model.layers.12.mlp.down_proj.weight.quant_map', 'model.layers.12.mlp.down_proj.weight.nested_absmax', 'model.layers.12.mlp.down_proj.weight.nested_quant_map', 'model.layers.12.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight.absmax', 'model.layers.13.self_attn.q_proj.weight.quant_map', 'model.layers.13.self_attn.q_proj.weight.nested_absmax', 'model.layers.13.self_attn.q_proj.weight.nested_quant_map', 'model.layers.13.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight.absmax', 'model.layers.13.self_attn.k_proj.weight.quant_map', 'model.layers.13.self_attn.k_proj.weight.nested_absmax', 'model.layers.13.self_attn.k_proj.weight.nested_quant_map', 'model.layers.13.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight.absmax', 'model.layers.13.self_attn.v_proj.weight.quant_map', 'model.layers.13.self_attn.v_proj.weight.nested_absmax', 'model.layers.13.self_attn.v_proj.weight.nested_quant_map', 'model.layers.13.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.o_proj.weight.absmax', 'model.layers.13.self_attn.o_proj.weight.quant_map', 'model.layers.13.self_attn.o_proj.weight.nested_absmax', 'model.layers.13.self_attn.o_proj.weight.nested_quant_map', 'model.layers.13.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.gate_proj.weight.absmax', 'model.layers.13.mlp.gate_proj.weight.quant_map', 'model.layers.13.mlp.gate_proj.weight.nested_absmax', 'model.layers.13.mlp.gate_proj.weight.nested_quant_map', 'model.layers.13.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.up_proj.weight.absmax', 'model.layers.13.mlp.up_proj.weight.quant_map', 'model.layers.13.mlp.up_proj.weight.nested_absmax', 'model.layers.13.mlp.up_proj.weight.nested_quant_map', 'model.layers.13.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.down_proj.weight.absmax', 'model.layers.13.mlp.down_proj.weight.quant_map', 'model.layers.13.mlp.down_proj.weight.nested_absmax', 'model.layers.13.mlp.down_proj.weight.nested_quant_map', 'model.layers.13.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight.absmax', 'model.layers.14.self_attn.q_proj.weight.quant_map', 'model.layers.14.self_attn.q_proj.weight.nested_absmax', 'model.layers.14.self_attn.q_proj.weight.nested_quant_map', 'model.layers.14.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.k_proj.weight.absmax', 'model.layers.14.self_attn.k_proj.weight.quant_map', 'model.layers.14.self_attn.k_proj.weight.nested_absmax', 'model.layers.14.self_attn.k_proj.weight.nested_quant_map', 'model.layers.14.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.weight.absmax', 'model.layers.14.self_attn.v_proj.weight.quant_map', 'model.layers.14.self_attn.v_proj.weight.nested_absmax', 'model.layers.14.self_attn.v_proj.weight.nested_quant_map', 'model.layers.14.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.o_proj.weight.absmax', 'model.layers.14.self_attn.o_proj.weight.quant_map', 'model.layers.14.self_attn.o_proj.weight.nested_absmax', 'model.layers.14.self_attn.o_proj.weight.nested_quant_map', 'model.layers.14.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.gate_proj.weight.absmax', 'model.layers.14.mlp.gate_proj.weight.quant_map', 'model.layers.14.mlp.gate_proj.weight.nested_absmax', 'model.layers.14.mlp.gate_proj.weight.nested_quant_map', 'model.layers.14.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.up_proj.weight.absmax', 'model.layers.14.mlp.up_proj.weight.quant_map', 'model.layers.14.mlp.up_proj.weight.nested_absmax', 'model.layers.14.mlp.up_proj.weight.nested_quant_map', 'model.layers.14.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.down_proj.weight.absmax', 'model.layers.14.mlp.down_proj.weight.quant_map', 'model.layers.14.mlp.down_proj.weight.nested_absmax', 'model.layers.14.mlp.down_proj.weight.nested_quant_map', 'model.layers.14.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.weight.absmax', 'model.layers.15.self_attn.q_proj.weight.quant_map', 'model.layers.15.self_attn.q_proj.weight.nested_absmax', 'model.layers.15.self_attn.q_proj.weight.nested_quant_map', 'model.layers.15.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.weight.absmax', 'model.layers.15.self_attn.k_proj.weight.quant_map', 'model.layers.15.self_attn.k_proj.weight.nested_absmax', 'model.layers.15.self_attn.k_proj.weight.nested_quant_map', 'model.layers.15.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.weight.absmax', 'model.layers.15.self_attn.v_proj.weight.quant_map', 'model.layers.15.self_attn.v_proj.weight.nested_absmax', 'model.layers.15.self_attn.v_proj.weight.nested_quant_map', 'model.layers.15.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.o_proj.weight.absmax', 'model.layers.15.self_attn.o_proj.weight.quant_map', 'model.layers.15.self_attn.o_proj.weight.nested_absmax', 'model.layers.15.self_attn.o_proj.weight.nested_quant_map', 'model.layers.15.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.gate_proj.weight.absmax', 'model.layers.15.mlp.gate_proj.weight.quant_map', 'model.layers.15.mlp.gate_proj.weight.nested_absmax', 'model.layers.15.mlp.gate_proj.weight.nested_quant_map', 'model.layers.15.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.up_proj.weight.absmax', 'model.layers.15.mlp.up_proj.weight.quant_map', 'model.layers.15.mlp.up_proj.weight.nested_absmax', 'model.layers.15.mlp.up_proj.weight.nested_quant_map', 'model.layers.15.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.down_proj.weight.absmax', 'model.layers.15.mlp.down_proj.weight.quant_map', 'model.layers.15.mlp.down_proj.weight.nested_absmax', 'model.layers.15.mlp.down_proj.weight.nested_quant_map', 'model.layers.15.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.q_proj.weight.absmax', 'model.layers.16.self_attn.q_proj.weight.quant_map', 'model.layers.16.self_attn.q_proj.weight.nested_absmax', 'model.layers.16.self_attn.q_proj.weight.nested_quant_map', 'model.layers.16.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight.absmax', 'model.layers.16.self_attn.k_proj.weight.quant_map', 'model.layers.16.self_attn.k_proj.weight.nested_absmax', 'model.layers.16.self_attn.k_proj.weight.nested_quant_map', 'model.layers.16.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.weight.absmax', 'model.layers.16.self_attn.v_proj.weight.quant_map', 'model.layers.16.self_attn.v_proj.weight.nested_absmax', 'model.layers.16.self_attn.v_proj.weight.nested_quant_map', 'model.layers.16.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.o_proj.weight.absmax', 'model.layers.16.self_attn.o_proj.weight.quant_map', 'model.layers.16.self_attn.o_proj.weight.nested_absmax', 'model.layers.16.self_attn.o_proj.weight.nested_quant_map', 'model.layers.16.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.gate_proj.weight.absmax', 'model.layers.16.mlp.gate_proj.weight.quant_map', 'model.layers.16.mlp.gate_proj.weight.nested_absmax', 'model.layers.16.mlp.gate_proj.weight.nested_quant_map', 'model.layers.16.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.up_proj.weight.absmax', 'model.layers.16.mlp.up_proj.weight.quant_map', 'model.layers.16.mlp.up_proj.weight.nested_absmax', 'model.layers.16.mlp.up_proj.weight.nested_quant_map', 'model.layers.16.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.down_proj.weight.absmax', 'model.layers.16.mlp.down_proj.weight.quant_map', 'model.layers.16.mlp.down_proj.weight.nested_absmax', 'model.layers.16.mlp.down_proj.weight.nested_quant_map', 'model.layers.16.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight.absmax', 'model.layers.17.self_attn.q_proj.weight.quant_map', 'model.layers.17.self_attn.q_proj.weight.nested_absmax', 'model.layers.17.self_attn.q_proj.weight.nested_quant_map', 'model.layers.17.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight.absmax', 'model.layers.17.self_attn.k_proj.weight.quant_map', 'model.layers.17.self_attn.k_proj.weight.nested_absmax', 'model.layers.17.self_attn.k_proj.weight.nested_quant_map', 'model.layers.17.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.v_proj.weight.absmax', 'model.layers.17.self_attn.v_proj.weight.quant_map', 'model.layers.17.self_attn.v_proj.weight.nested_absmax', 'model.layers.17.self_attn.v_proj.weight.nested_quant_map', 'model.layers.17.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.o_proj.weight.absmax', 'model.layers.17.self_attn.o_proj.weight.quant_map', 'model.layers.17.self_attn.o_proj.weight.nested_absmax', 'model.layers.17.self_attn.o_proj.weight.nested_quant_map', 'model.layers.17.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.gate_proj.weight.absmax', 'model.layers.17.mlp.gate_proj.weight.quant_map', 'model.layers.17.mlp.gate_proj.weight.nested_absmax', 'model.layers.17.mlp.gate_proj.weight.nested_quant_map', 'model.layers.17.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.up_proj.weight.absmax', 'model.layers.17.mlp.up_proj.weight.quant_map', 'model.layers.17.mlp.up_proj.weight.nested_absmax', 'model.layers.17.mlp.up_proj.weight.nested_quant_map', 'model.layers.17.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.down_proj.weight.absmax', 'model.layers.17.mlp.down_proj.weight.quant_map', 'model.layers.17.mlp.down_proj.weight.nested_absmax', 'model.layers.17.mlp.down_proj.weight.nested_quant_map', 'model.layers.17.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.q_proj.weight.absmax', 'model.layers.18.self_attn.q_proj.weight.quant_map', 'model.layers.18.self_attn.q_proj.weight.nested_absmax', 'model.layers.18.self_attn.q_proj.weight.nested_quant_map', 'model.layers.18.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.k_proj.weight.absmax', 'model.layers.18.self_attn.k_proj.weight.quant_map', 'model.layers.18.self_attn.k_proj.weight.nested_absmax', 'model.layers.18.self_attn.k_proj.weight.nested_quant_map', 'model.layers.18.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.v_proj.weight.absmax', 'model.layers.18.self_attn.v_proj.weight.quant_map', 'model.layers.18.self_attn.v_proj.weight.nested_absmax', 'model.layers.18.self_attn.v_proj.weight.nested_quant_map', 'model.layers.18.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.o_proj.weight.absmax', 'model.layers.18.self_attn.o_proj.weight.quant_map', 'model.layers.18.self_attn.o_proj.weight.nested_absmax', 'model.layers.18.self_attn.o_proj.weight.nested_quant_map', 'model.layers.18.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.gate_proj.weight.absmax', 'model.layers.18.mlp.gate_proj.weight.quant_map', 'model.layers.18.mlp.gate_proj.weight.nested_absmax', 'model.layers.18.mlp.gate_proj.weight.nested_quant_map', 'model.layers.18.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.up_proj.weight.absmax', 'model.layers.18.mlp.up_proj.weight.quant_map', 'model.layers.18.mlp.up_proj.weight.nested_absmax', 'model.layers.18.mlp.up_proj.weight.nested_quant_map', 'model.layers.18.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.down_proj.weight.absmax', 'model.layers.18.mlp.down_proj.weight.quant_map', 'model.layers.18.mlp.down_proj.weight.nested_absmax', 'model.layers.18.mlp.down_proj.weight.nested_quant_map', 'model.layers.18.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.weight.absmax', 'model.layers.19.self_attn.q_proj.weight.quant_map', 'model.layers.19.self_attn.q_proj.weight.nested_absmax', 'model.layers.19.self_attn.q_proj.weight.nested_quant_map', 'model.layers.19.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.k_proj.weight.absmax', 'model.layers.19.self_attn.k_proj.weight.quant_map', 'model.layers.19.self_attn.k_proj.weight.nested_absmax', 'model.layers.19.self_attn.k_proj.weight.nested_quant_map', 'model.layers.19.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.weight.absmax', 'model.layers.19.self_attn.v_proj.weight.quant_map', 'model.layers.19.self_attn.v_proj.weight.nested_absmax', 'model.layers.19.self_attn.v_proj.weight.nested_quant_map', 'model.layers.19.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.o_proj.weight.absmax', 'model.layers.19.self_attn.o_proj.weight.quant_map', 'model.layers.19.self_attn.o_proj.weight.nested_absmax', 'model.layers.19.self_attn.o_proj.weight.nested_quant_map', 'model.layers.19.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.gate_proj.weight.absmax', 'model.layers.19.mlp.gate_proj.weight.quant_map', 'model.layers.19.mlp.gate_proj.weight.nested_absmax', 'model.layers.19.mlp.gate_proj.weight.nested_quant_map', 'model.layers.19.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.up_proj.weight.absmax', 'model.layers.19.mlp.up_proj.weight.quant_map', 'model.layers.19.mlp.up_proj.weight.nested_absmax', 'model.layers.19.mlp.up_proj.weight.nested_quant_map', 'model.layers.19.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.down_proj.weight.absmax', 'model.layers.19.mlp.down_proj.weight.quant_map', 'model.layers.19.mlp.down_proj.weight.nested_absmax', 'model.layers.19.mlp.down_proj.weight.nested_quant_map', 'model.layers.19.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight.absmax', 'model.layers.20.self_attn.q_proj.weight.quant_map', 'model.layers.20.self_attn.q_proj.weight.nested_absmax', 'model.layers.20.self_attn.q_proj.weight.nested_quant_map', 'model.layers.20.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.k_proj.weight.absmax', 'model.layers.20.self_attn.k_proj.weight.quant_map', 'model.layers.20.self_attn.k_proj.weight.nested_absmax', 'model.layers.20.self_attn.k_proj.weight.nested_quant_map', 'model.layers.20.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight.absmax', 'model.layers.20.self_attn.v_proj.weight.quant_map', 'model.layers.20.self_attn.v_proj.weight.nested_absmax', 'model.layers.20.self_attn.v_proj.weight.nested_quant_map', 'model.layers.20.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.o_proj.weight.absmax', 'model.layers.20.self_attn.o_proj.weight.quant_map', 'model.layers.20.self_attn.o_proj.weight.nested_absmax', 'model.layers.20.self_attn.o_proj.weight.nested_quant_map', 'model.layers.20.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.gate_proj.weight.absmax', 'model.layers.20.mlp.gate_proj.weight.quant_map', 'model.layers.20.mlp.gate_proj.weight.nested_absmax', 'model.layers.20.mlp.gate_proj.weight.nested_quant_map', 'model.layers.20.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.up_proj.weight.absmax', 'model.layers.20.mlp.up_proj.weight.quant_map', 'model.layers.20.mlp.up_proj.weight.nested_absmax', 'model.layers.20.mlp.up_proj.weight.nested_quant_map', 'model.layers.20.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.down_proj.weight.absmax', 'model.layers.20.mlp.down_proj.weight.quant_map', 'model.layers.20.mlp.down_proj.weight.nested_absmax', 'model.layers.20.mlp.down_proj.weight.nested_quant_map', 'model.layers.20.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.weight.absmax', 'model.layers.21.self_attn.q_proj.weight.quant_map', 'model.layers.21.self_attn.q_proj.weight.nested_absmax', 'model.layers.21.self_attn.q_proj.weight.nested_quant_map', 'model.layers.21.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.weight.absmax', 'model.layers.21.self_attn.k_proj.weight.quant_map', 'model.layers.21.self_attn.k_proj.weight.nested_absmax', 'model.layers.21.self_attn.k_proj.weight.nested_quant_map', 'model.layers.21.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.v_proj.weight.absmax', 'model.layers.21.self_attn.v_proj.weight.quant_map', 'model.layers.21.self_attn.v_proj.weight.nested_absmax', 'model.layers.21.self_attn.v_proj.weight.nested_quant_map', 'model.layers.21.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.o_proj.weight.absmax', 'model.layers.21.self_attn.o_proj.weight.quant_map', 'model.layers.21.self_attn.o_proj.weight.nested_absmax', 'model.layers.21.self_attn.o_proj.weight.nested_quant_map', 'model.layers.21.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight.absmax', 'model.layers.21.mlp.gate_proj.weight.quant_map', 'model.layers.21.mlp.gate_proj.weight.nested_absmax', 'model.layers.21.mlp.gate_proj.weight.nested_quant_map', 'model.layers.21.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.up_proj.weight.absmax', 'model.layers.21.mlp.up_proj.weight.quant_map', 'model.layers.21.mlp.up_proj.weight.nested_absmax', 'model.layers.21.mlp.up_proj.weight.nested_quant_map', 'model.layers.21.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.down_proj.weight.absmax', 'model.layers.21.mlp.down_proj.weight.quant_map', 'model.layers.21.mlp.down_proj.weight.nested_absmax', 'model.layers.21.mlp.down_proj.weight.nested_quant_map', 'model.layers.21.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.q_proj.weight.absmax', 'model.layers.22.self_attn.q_proj.weight.quant_map', 'model.layers.22.self_attn.q_proj.weight.nested_absmax', 'model.layers.22.self_attn.q_proj.weight.nested_quant_map', 'model.layers.22.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.weight.absmax', 'model.layers.22.self_attn.k_proj.weight.quant_map', 'model.layers.22.self_attn.k_proj.weight.nested_absmax', 'model.layers.22.self_attn.k_proj.weight.nested_quant_map', 'model.layers.22.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight.absmax', 'model.layers.22.self_attn.v_proj.weight.quant_map', 'model.layers.22.self_attn.v_proj.weight.nested_absmax', 'model.layers.22.self_attn.v_proj.weight.nested_quant_map', 'model.layers.22.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.o_proj.weight.absmax', 'model.layers.22.self_attn.o_proj.weight.quant_map', 'model.layers.22.self_attn.o_proj.weight.nested_absmax', 'model.layers.22.self_attn.o_proj.weight.nested_quant_map', 'model.layers.22.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.gate_proj.weight.absmax', 'model.layers.22.mlp.gate_proj.weight.quant_map', 'model.layers.22.mlp.gate_proj.weight.nested_absmax', 'model.layers.22.mlp.gate_proj.weight.nested_quant_map', 'model.layers.22.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.up_proj.weight.absmax', 'model.layers.22.mlp.up_proj.weight.quant_map', 'model.layers.22.mlp.up_proj.weight.nested_absmax', 'model.layers.22.mlp.up_proj.weight.nested_quant_map', 'model.layers.22.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.down_proj.weight.absmax', 'model.layers.22.mlp.down_proj.weight.quant_map', 'model.layers.22.mlp.down_proj.weight.nested_absmax', 'model.layers.22.mlp.down_proj.weight.nested_quant_map', 'model.layers.22.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.weight.absmax', 'model.layers.23.self_attn.q_proj.weight.quant_map', 'model.layers.23.self_attn.q_proj.weight.nested_absmax', 'model.layers.23.self_attn.q_proj.weight.nested_quant_map', 'model.layers.23.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight.absmax', 'model.layers.23.self_attn.k_proj.weight.quant_map', 'model.layers.23.self_attn.k_proj.weight.nested_absmax', 'model.layers.23.self_attn.k_proj.weight.nested_quant_map', 'model.layers.23.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.v_proj.weight.absmax', 'model.layers.23.self_attn.v_proj.weight.quant_map', 'model.layers.23.self_attn.v_proj.weight.nested_absmax', 'model.layers.23.self_attn.v_proj.weight.nested_quant_map', 'model.layers.23.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.o_proj.weight.absmax', 'model.layers.23.self_attn.o_proj.weight.quant_map', 'model.layers.23.self_attn.o_proj.weight.nested_absmax', 'model.layers.23.self_attn.o_proj.weight.nested_quant_map', 'model.layers.23.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.gate_proj.weight.absmax', 'model.layers.23.mlp.gate_proj.weight.quant_map', 'model.layers.23.mlp.gate_proj.weight.nested_absmax', 'model.layers.23.mlp.gate_proj.weight.nested_quant_map', 'model.layers.23.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.up_proj.weight.absmax', 'model.layers.23.mlp.up_proj.weight.quant_map', 'model.layers.23.mlp.up_proj.weight.nested_absmax', 'model.layers.23.mlp.up_proj.weight.nested_quant_map', 'model.layers.23.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.down_proj.weight.absmax', 'model.layers.23.mlp.down_proj.weight.quant_map', 'model.layers.23.mlp.down_proj.weight.nested_absmax', 'model.layers.23.mlp.down_proj.weight.nested_quant_map', 'model.layers.23.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.q_proj.weight.absmax', 'model.layers.24.self_attn.q_proj.weight.quant_map', 'model.layers.24.self_attn.q_proj.weight.nested_absmax', 'model.layers.24.self_attn.q_proj.weight.nested_quant_map', 'model.layers.24.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.k_proj.weight.absmax', 'model.layers.24.self_attn.k_proj.weight.quant_map', 'model.layers.24.self_attn.k_proj.weight.nested_absmax', 'model.layers.24.self_attn.k_proj.weight.nested_quant_map', 'model.layers.24.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.weight.absmax', 'model.layers.24.self_attn.v_proj.weight.quant_map', 'model.layers.24.self_attn.v_proj.weight.nested_absmax', 'model.layers.24.self_attn.v_proj.weight.nested_quant_map', 'model.layers.24.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.o_proj.weight.absmax', 'model.layers.24.self_attn.o_proj.weight.quant_map', 'model.layers.24.self_attn.o_proj.weight.nested_absmax', 'model.layers.24.self_attn.o_proj.weight.nested_quant_map', 'model.layers.24.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.gate_proj.weight.absmax', 'model.layers.24.mlp.gate_proj.weight.quant_map', 'model.layers.24.mlp.gate_proj.weight.nested_absmax', 'model.layers.24.mlp.gate_proj.weight.nested_quant_map', 'model.layers.24.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.up_proj.weight.absmax', 'model.layers.24.mlp.up_proj.weight.quant_map', 'model.layers.24.mlp.up_proj.weight.nested_absmax', 'model.layers.24.mlp.up_proj.weight.nested_quant_map', 'model.layers.24.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.down_proj.weight.absmax', 'model.layers.24.mlp.down_proj.weight.quant_map', 'model.layers.24.mlp.down_proj.weight.nested_absmax', 'model.layers.24.mlp.down_proj.weight.nested_quant_map', 'model.layers.24.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.q_proj.weight.absmax', 'model.layers.25.self_attn.q_proj.weight.quant_map', 'model.layers.25.self_attn.q_proj.weight.nested_absmax', 'model.layers.25.self_attn.q_proj.weight.nested_quant_map', 'model.layers.25.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.k_proj.weight.absmax', 'model.layers.25.self_attn.k_proj.weight.quant_map', 'model.layers.25.self_attn.k_proj.weight.nested_absmax', 'model.layers.25.self_attn.k_proj.weight.nested_quant_map', 'model.layers.25.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.v_proj.weight.absmax', 'model.layers.25.self_attn.v_proj.weight.quant_map', 'model.layers.25.self_attn.v_proj.weight.nested_absmax', 'model.layers.25.self_attn.v_proj.weight.nested_quant_map', 'model.layers.25.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.o_proj.weight.absmax', 'model.layers.25.self_attn.o_proj.weight.quant_map', 'model.layers.25.self_attn.o_proj.weight.nested_absmax', 'model.layers.25.self_attn.o_proj.weight.nested_quant_map', 'model.layers.25.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.gate_proj.weight.absmax', 'model.layers.25.mlp.gate_proj.weight.quant_map', 'model.layers.25.mlp.gate_proj.weight.nested_absmax', 'model.layers.25.mlp.gate_proj.weight.nested_quant_map', 'model.layers.25.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.up_proj.weight.absmax', 'model.layers.25.mlp.up_proj.weight.quant_map', 'model.layers.25.mlp.up_proj.weight.nested_absmax', 'model.layers.25.mlp.up_proj.weight.nested_quant_map', 'model.layers.25.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.down_proj.weight.absmax', 'model.layers.25.mlp.down_proj.weight.quant_map', 'model.layers.25.mlp.down_proj.weight.nested_absmax', 'model.layers.25.mlp.down_proj.weight.nested_quant_map', 'model.layers.25.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.q_proj.weight.absmax', 'model.layers.26.self_attn.q_proj.weight.quant_map', 'model.layers.26.self_attn.q_proj.weight.nested_absmax', 'model.layers.26.self_attn.q_proj.weight.nested_quant_map', 'model.layers.26.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.weight.absmax', 'model.layers.26.self_attn.k_proj.weight.quant_map', 'model.layers.26.self_attn.k_proj.weight.nested_absmax', 'model.layers.26.self_attn.k_proj.weight.nested_quant_map', 'model.layers.26.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.v_proj.weight.absmax', 'model.layers.26.self_attn.v_proj.weight.quant_map', 'model.layers.26.self_attn.v_proj.weight.nested_absmax', 'model.layers.26.self_attn.v_proj.weight.nested_quant_map', 'model.layers.26.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.o_proj.weight.absmax', 'model.layers.26.self_attn.o_proj.weight.quant_map', 'model.layers.26.self_attn.o_proj.weight.nested_absmax', 'model.layers.26.self_attn.o_proj.weight.nested_quant_map', 'model.layers.26.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.gate_proj.weight.absmax', 'model.layers.26.mlp.gate_proj.weight.quant_map', 'model.layers.26.mlp.gate_proj.weight.nested_absmax', 'model.layers.26.mlp.gate_proj.weight.nested_quant_map', 'model.layers.26.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.up_proj.weight.absmax', 'model.layers.26.mlp.up_proj.weight.quant_map', 'model.layers.26.mlp.up_proj.weight.nested_absmax', 'model.layers.26.mlp.up_proj.weight.nested_quant_map', 'model.layers.26.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.down_proj.weight.absmax', 'model.layers.26.mlp.down_proj.weight.quant_map', 'model.layers.26.mlp.down_proj.weight.nested_absmax', 'model.layers.26.mlp.down_proj.weight.nested_quant_map', 'model.layers.26.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.q_proj.weight.absmax', 'model.layers.27.self_attn.q_proj.weight.quant_map', 'model.layers.27.self_attn.q_proj.weight.nested_absmax', 'model.layers.27.self_attn.q_proj.weight.nested_quant_map', 'model.layers.27.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.k_proj.weight.absmax', 'model.layers.27.self_attn.k_proj.weight.quant_map', 'model.layers.27.self_attn.k_proj.weight.nested_absmax', 'model.layers.27.self_attn.k_proj.weight.nested_quant_map', 'model.layers.27.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.weight.absmax', 'model.layers.27.self_attn.v_proj.weight.quant_map', 'model.layers.27.self_attn.v_proj.weight.nested_absmax', 'model.layers.27.self_attn.v_proj.weight.nested_quant_map', 'model.layers.27.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.o_proj.weight.absmax', 'model.layers.27.self_attn.o_proj.weight.quant_map', 'model.layers.27.self_attn.o_proj.weight.nested_absmax', 'model.layers.27.self_attn.o_proj.weight.nested_quant_map', 'model.layers.27.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.gate_proj.weight.absmax', 'model.layers.27.mlp.gate_proj.weight.quant_map', 'model.layers.27.mlp.gate_proj.weight.nested_absmax', 'model.layers.27.mlp.gate_proj.weight.nested_quant_map', 'model.layers.27.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.up_proj.weight.absmax', 'model.layers.27.mlp.up_proj.weight.quant_map', 'model.layers.27.mlp.up_proj.weight.nested_absmax', 'model.layers.27.mlp.up_proj.weight.nested_quant_map', 'model.layers.27.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.down_proj.weight.absmax', 'model.layers.27.mlp.down_proj.weight.quant_map', 'model.layers.27.mlp.down_proj.weight.nested_absmax', 'model.layers.27.mlp.down_proj.weight.nested_quant_map', 'model.layers.27.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.q_proj.weight.absmax', 'model.layers.28.self_attn.q_proj.weight.quant_map', 'model.layers.28.self_attn.q_proj.weight.nested_absmax', 'model.layers.28.self_attn.q_proj.weight.nested_quant_map', 'model.layers.28.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.k_proj.weight.absmax', 'model.layers.28.self_attn.k_proj.weight.quant_map', 'model.layers.28.self_attn.k_proj.weight.nested_absmax', 'model.layers.28.self_attn.k_proj.weight.nested_quant_map', 'model.layers.28.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.28.self_attn.v_proj.weight.absmax', 'model.layers.28.self_attn.v_proj.weight.quant_map', 'model.layers.28.self_attn.v_proj.weight.nested_absmax', 'model.layers.28.self_attn.v_proj.weight.nested_quant_map', 'model.layers.28.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.o_proj.weight.absmax', 'model.layers.28.self_attn.o_proj.weight.quant_map', 'model.layers.28.self_attn.o_proj.weight.nested_absmax', 'model.layers.28.self_attn.o_proj.weight.nested_quant_map', 'model.layers.28.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.gate_proj.weight.absmax', 'model.layers.28.mlp.gate_proj.weight.quant_map', 'model.layers.28.mlp.gate_proj.weight.nested_absmax', 'model.layers.28.mlp.gate_proj.weight.nested_quant_map', 'model.layers.28.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.up_proj.weight.absmax', 'model.layers.28.mlp.up_proj.weight.quant_map', 'model.layers.28.mlp.up_proj.weight.nested_absmax', 'model.layers.28.mlp.up_proj.weight.nested_quant_map', 'model.layers.28.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.down_proj.weight.absmax', 'model.layers.28.mlp.down_proj.weight.quant_map', 'model.layers.28.mlp.down_proj.weight.nested_absmax', 'model.layers.28.mlp.down_proj.weight.nested_quant_map', 'model.layers.28.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.q_proj.weight.absmax', 'model.layers.29.self_attn.q_proj.weight.quant_map', 'model.layers.29.self_attn.q_proj.weight.nested_absmax', 'model.layers.29.self_attn.q_proj.weight.nested_quant_map', 'model.layers.29.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.k_proj.weight.absmax', 'model.layers.29.self_attn.k_proj.weight.quant_map', 'model.layers.29.self_attn.k_proj.weight.nested_absmax', 'model.layers.29.self_attn.k_proj.weight.nested_quant_map', 'model.layers.29.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.v_proj.weight.absmax', 'model.layers.29.self_attn.v_proj.weight.quant_map', 'model.layers.29.self_attn.v_proj.weight.nested_absmax', 'model.layers.29.self_attn.v_proj.weight.nested_quant_map', 'model.layers.29.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.o_proj.weight.absmax', 'model.layers.29.self_attn.o_proj.weight.quant_map', 'model.layers.29.self_attn.o_proj.weight.nested_absmax', 'model.layers.29.self_attn.o_proj.weight.nested_quant_map', 'model.layers.29.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.gate_proj.weight.absmax', 'model.layers.29.mlp.gate_proj.weight.quant_map', 'model.layers.29.mlp.gate_proj.weight.nested_absmax', 'model.layers.29.mlp.gate_proj.weight.nested_quant_map', 'model.layers.29.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.up_proj.weight.absmax', 'model.layers.29.mlp.up_proj.weight.quant_map', 'model.layers.29.mlp.up_proj.weight.nested_absmax', 'model.layers.29.mlp.up_proj.weight.nested_quant_map', 'model.layers.29.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.down_proj.weight.absmax', 'model.layers.29.mlp.down_proj.weight.quant_map', 'model.layers.29.mlp.down_proj.weight.nested_absmax', 'model.layers.29.mlp.down_proj.weight.nested_quant_map', 'model.layers.29.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.weight.absmax', 'model.layers.30.self_attn.q_proj.weight.quant_map', 'model.layers.30.self_attn.q_proj.weight.nested_absmax', 'model.layers.30.self_attn.q_proj.weight.nested_quant_map', 'model.layers.30.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.k_proj.weight.absmax', 'model.layers.30.self_attn.k_proj.weight.quant_map', 'model.layers.30.self_attn.k_proj.weight.nested_absmax', 'model.layers.30.self_attn.k_proj.weight.nested_quant_map', 'model.layers.30.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.v_proj.weight.absmax', 'model.layers.30.self_attn.v_proj.weight.quant_map', 'model.layers.30.self_attn.v_proj.weight.nested_absmax', 'model.layers.30.self_attn.v_proj.weight.nested_quant_map', 'model.layers.30.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.o_proj.weight.absmax', 'model.layers.30.self_attn.o_proj.weight.quant_map', 'model.layers.30.self_attn.o_proj.weight.nested_absmax', 'model.layers.30.self_attn.o_proj.weight.nested_quant_map', 'model.layers.30.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.gate_proj.weight.absmax', 'model.layers.30.mlp.gate_proj.weight.quant_map', 'model.layers.30.mlp.gate_proj.weight.nested_absmax', 'model.layers.30.mlp.gate_proj.weight.nested_quant_map', 'model.layers.30.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.up_proj.weight.absmax', 'model.layers.30.mlp.up_proj.weight.quant_map', 'model.layers.30.mlp.up_proj.weight.nested_absmax', 'model.layers.30.mlp.up_proj.weight.nested_quant_map', 'model.layers.30.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.down_proj.weight.absmax', 'model.layers.30.mlp.down_proj.weight.quant_map', 'model.layers.30.mlp.down_proj.weight.nested_absmax', 'model.layers.30.mlp.down_proj.weight.nested_quant_map', 'model.layers.30.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.q_proj.weight.absmax', 'model.layers.31.self_attn.q_proj.weight.quant_map', 'model.layers.31.self_attn.q_proj.weight.nested_absmax', 'model.layers.31.self_attn.q_proj.weight.nested_quant_map', 'model.layers.31.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.weight.absmax', 'model.layers.31.self_attn.k_proj.weight.quant_map', 'model.layers.31.self_attn.k_proj.weight.nested_absmax', 'model.layers.31.self_attn.k_proj.weight.nested_quant_map', 'model.layers.31.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.weight.absmax', 'model.layers.31.self_attn.v_proj.weight.quant_map', 'model.layers.31.self_attn.v_proj.weight.nested_absmax', 'model.layers.31.self_attn.v_proj.weight.nested_quant_map', 'model.layers.31.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.o_proj.weight.absmax', 'model.layers.31.self_attn.o_proj.weight.quant_map', 'model.layers.31.self_attn.o_proj.weight.nested_absmax', 'model.layers.31.self_attn.o_proj.weight.nested_quant_map', 'model.layers.31.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.gate_proj.weight.absmax', 'model.layers.31.mlp.gate_proj.weight.quant_map', 'model.layers.31.mlp.gate_proj.weight.nested_absmax', 'model.layers.31.mlp.gate_proj.weight.nested_quant_map', 'model.layers.31.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.up_proj.weight.absmax', 'model.layers.31.mlp.up_proj.weight.quant_map', 'model.layers.31.mlp.up_proj.weight.nested_absmax', 'model.layers.31.mlp.up_proj.weight.nested_quant_map', 'model.layers.31.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.down_proj.weight.absmax', 'model.layers.31.mlp.down_proj.weight.quant_map', 'model.layers.31.mlp.down_proj.weight.nested_absmax', 'model.layers.31.mlp.down_proj.weight.nested_quant_map', 'model.layers.31.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight'])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56c281a7-cb38-4bba-8729-b18a48493c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params : 16777216 || All params : 3517198336 || Trainable % : 1677721600\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r= 16,\n",
    "    lora_alpha=32,\n",
    "    target_modules = [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\"],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = 'none',\n",
    "    task_type = 'CAUSAL_LM'\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95308f30-2d6e-42b7-af29-7f9b7e7b187c",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50bd96b6-6121-42c1-8eee-6a64bebb311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Locutusque/function-calling-chatml\")\n",
    "# data = data.map(lambda samples: tokenizer(samples['quote']), batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a33be811-85b2-43d6-8dd9-54eb277df910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['system_message', 'function_description', 'conversations'],\n",
      "        num_rows: 112960\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6d5e42-863e-4bd9-bf10-bde21a17682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, response_lengths):\n",
    "        self.encodings = encodings\n",
    "        self.response_length = response_lengths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "\n",
    "        #Set labels to be the same as input ids\n",
    "        item[\"labels\"] = item['input_ids'].clone()\n",
    "\n",
    "        #Shift labels to the left and replace the last posistion with EOS token\n",
    "        item[\"labels\"][:-1] = item['input_ids'][1:]\n",
    "        item[\"labels\"][-1] = 2 #Replace last position with EOS token ID\n",
    "\n",
    "        # Create a loss mask\n",
    "        response_start_position = item['input_ids'].shape[0] - self.response_lengths[idx]\n",
    "        item['loss_mask'] = torch.zeros_like(item['input_ids'])\n",
    "        item['loss_mask'][response_start_position] = 1\n",
    "\n",
    "        #Create a new tensor for the shifted loss mask\n",
    "        shifted_loss_mask = torch.cat([item['loss_mask'][1:], torch.tensor([1])])\n",
    "        item['loss_mask'] = shifted_loss_mask\n",
    "\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bb7ddf5-d2ca-43c6-8885-0f1ca82eda5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"name\": \"get_exchange_rate\",\\n    \"description\": \"Get the exchange rate between two currencies\",\\n    \"parameters\": {\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"base_currency\": {\\n                \"type\": \"string\",\\n                \"description\": \"The currency to convert from\"\\n            },\\n            \"target_currency\": {\\n                \"type\": \"string\",\\n                \"description\": \"The currency to convert to\"\\n            }\\n        },\\n        \"required\": [\\n            \"base_currency\",\\n            \"target_currency\"\\n        ]\\n    }\\n}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][\"function_description\"][0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c54b3674-02e0-413e-a0b4-2b7de9936571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, tokenizer):\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<<SYS>>\\n\\n\"\n",
    "\n",
    "    formatted_dataset = []\n",
    "    for row in dataset:\n",
    "        # Combine system and function into one system prompt\n",
    "        system_prompt = f\"{B_SYS}{row['system_message'].strip()}\\n\\n{row['function_description'].strip()}{E_SYS}\"\n",
    "\n",
    "        conversation = row[\"conversations\"]\n",
    "        if isinstance(conversation, str):\n",
    "            import ast\n",
    "            conversation = ast.literal_eval(conversation)\n",
    "\n",
    "        # Process only the last user + assistant turn for SFT\n",
    "        last_user = None\n",
    "        last_assistant = None\n",
    "        for entry in reversed(conversation):\n",
    "            if entry[\"from\"] == \"gpt\" and last_assistant is None:\n",
    "                last_assistant = entry[\"value\"].strip()\n",
    "            elif entry[\"from\"] == \"human\" and last_user is None:\n",
    "                last_user = entry[\"value\"].strip()\n",
    "            if last_user and last_assistant:\n",
    "                break\n",
    "\n",
    "        if last_user is None or last_assistant is None:\n",
    "            continue  # skip incomplete samples\n",
    "\n",
    "        # Prepare ChatML-style input\n",
    "        input_text = f\"{B_INST}{system_prompt}{last_user.strip()}{E_INST} {last_assistant.strip()}\"\n",
    "        response_text = last_assistant.strip()\n",
    "\n",
    "        formatted_dataset.append({\n",
    "            \"input_text\": input_text,\n",
    "            \"response_text\": response_text\n",
    "        })\n",
    "\n",
    "    # Tokenize inputs\n",
    "    input_texts = [d[\"input_text\"] for d in formatted_dataset]\n",
    "    response_texts = [d[\"response_text\"] for d in formatted_dataset]\n",
    "\n",
    "    encodings = tokenizer(input_texts, truncation=True, padding=True, max_length=2048, return_tensors=\"pt\")\n",
    "    response_lengths = [len(tokenizer.encode(resp, truncation=True, max_length=tokenizer.model_max_length)) for resp in response_texts]\n",
    "\n",
    "    return TextDataset(encodings, response_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "445b7bb4-dc48-4293-9eef-9f6b9c87207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(dataset, tokenizer):\n",
    "#     B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "#     B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<<SYS>>\\n\\n\"\n",
    "\n",
    "#     formatted_dataset = dataset.map(\n",
    "#         lambda x : {\n",
    "#             \"input_text\" : \"\".join([\n",
    "#                 f\"{B_INST}{B_SYS}{x['system_message'].strip()}{E_SYS}\",\n",
    "#                 f\"{\"\".join(x['function_description']).strip()}{E_INST}\\n\\n\",\n",
    "#                 f\"{\"\".join(x['conversations']).strip()}\" #Appending EOS token in text data\n",
    "#             ]),\n",
    "\n",
    "#             \"response_text\" : \"\".join([\n",
    "#                 f\"{\"\".join(x['conversations']).strip()}\" #Appending EOS token in text data\n",
    "#             ])\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     encodings = tokenizer([dialogue['input_text'] for dialog in formatted_dataset], Truncation = True)\n",
    "#     response_lengths = [len(tokenizer.encode(dialogue[\"response_text\"], truncation = True, max_length = 84))]\n",
    "\n",
    "#     text_dataset = TextDataset(encodings, response_lengths)\n",
    "\n",
    "#     return text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f403f100-c7e0-4bbd-8050-a6e67d56381a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system_message', 'function_description', 'conversations'],\n",
       "    num_rows: 112960\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb67a97-0628-4999-b46e-5b9e2ca08f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = prepare_dataset(data['train'], tokenizer)\n",
    "test_dataset = prepare_dataset(data['test'], tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
