{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fed83ec6-7af8-4f31-986e-fcb42f4440bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "import warnings\n",
    "import faiss\n",
    "\n",
    "import os\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1474f7f9-5855-49b3-9dd5-ee47ec70f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagLlamaFAISS:\n",
    "    def __init__(self,\n",
    "                 context_window=4096,\n",
    "                 max_new_tokens=64,\n",
    "                 generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "                 tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                 model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                 device_map=\"auto\",\n",
    "                 model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True},\n",
    "                 similarity_top_k=3,\n",
    "                 persist_dir=\"storage\",\n",
    "                 faiss_index_file=\"faiss_index.idx\",\n",
    "                 system_prompt=\"\"\"You are a human being that is trying to converse with an Alzheimer's patient. Use the memories in the data and respond naturally.\"\"\"):\n",
    "\n",
    "        self.context_window = context_window\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.generate_kwargs = generate_kwargs\n",
    "        self.system_prompt = system_prompt\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.model_name = model_name\n",
    "        self.device_map = device_map\n",
    "        self.model_kwargs = model_kwargs\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.persist_dir = persist_dir\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "\n",
    "        self.query_engine = None\n",
    "        self.documents = None\n",
    "        self.index = None\n",
    "        self.llm = None\n",
    "        self.embed_model = None\n",
    "        self.vector_store = None\n",
    "\n",
    "    def load_model(self):\n",
    "        self.llm = HuggingFaceLLM(\n",
    "            context_window=self.context_window,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            generate_kwargs=self.generate_kwargs,\n",
    "            system_prompt=self.system_prompt,\n",
    "            tokenizer_name=self.tokenizer_name,\n",
    "            model_name=self.model_name,\n",
    "            device_map=self.device_map,\n",
    "            model_kwargs=self.model_kwargs,\n",
    "        )\n",
    "\n",
    "    def load_data(self, data_path=\"./data\"):\n",
    "        self.documents = SimpleDirectoryReader(data_path).load_data()\n",
    "        if not self.documents:\n",
    "            raise ValueError(\"No documents found at specified path.\")\n",
    "        print(f\"Loaded {len(self.documents)} documents.\")\n",
    "\n",
    "    def prepare(self, embedding_model=\"sentence-transformers/all-mpnet-base-v2\", data_path=\"./data\"):\n",
    "        self.load_model()\n",
    "        self.load_data(data_path)\n",
    "    \n",
    "        hf_embed_model = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        self.embed_model = LangchainEmbedding(hf_embed_model)\n",
    "    \n",
    "        Settings.llm = self.llm\n",
    "        Settings.embed_model = self.embed_model\n",
    "        Settings.chunk_size = 1024\n",
    "    \n",
    "        if not os.path.exists(self.persist_dir):\n",
    "            os.makedirs(self.persist_dir)\n",
    "    \n",
    "        # Check if persisted index exists\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)\n",
    "    \n",
    "        # Try loading index from storage\n",
    "        try:\n",
    "            self.index = load_index_from_storage(storage_context)\n",
    "            print(\"Loaded existing FAISS index from disk...\")\n",
    "        except Exception:\n",
    "            print(\"Building new FAISS index from documents...\")\n",
    "    \n",
    "            # Create FAISS index inside FaissVectorStore automatically\n",
    "            self.index = VectorStoreIndex.from_documents(\n",
    "                self.documents,\n",
    "                storage_context=storage_context\n",
    "            )\n",
    "            # Persist the storage context, which includes the vector store & index files\n",
    "            storage_context.persist()\n",
    "    \n",
    "        self.query_engine = self.index.as_query_engine(similarity_top_k=self.similarity_top_k)\n",
    "\n",
    "    def call(self, query: str):\n",
    "        if self.query_engine is None:\n",
    "            raise RuntimeError(\"Model not prepared. Call `prepare()` first.\")\n",
    "        print(f\"Querying: {query}\")\n",
    "        response = self.query_engine.query(query)\n",
    "        return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "561cd885-b981-4e72-bc18-496a369cd82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Rag_Llama:\n",
    "#     def __init__(self,\n",
    "#                 context_window=4096,\n",
    "#                 max_new_tokens=256,\n",
    "#                 generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "#                 system_prompt=\"\"\"\"\"\",\n",
    "#                 tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#                 model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#                 device_map=\"cuda:0\",\n",
    "#                 model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}):\n",
    "        \n",
    "#         self.context_window= context_window\n",
    "#         self.max_new_tokens= max_new_tokens\n",
    "#         self.generate_kwargs= generate_kwargs\n",
    "#         self.system_prompt=system_prompt\n",
    "#         # query_wrapper_prompt=query_wrapper_prompt,\n",
    "#         self.tokenizer_name= tokenizer_name\n",
    "#         self.model_name= model_name\n",
    "#         self.device_map= device_map\n",
    "#         # uncomment this if using CUDA to reduce memory usage\n",
    "#         self.model_kwargs= model_kwargs\n",
    "        \n",
    "#         self.query_engine = None\n",
    "#         self.documents = None\n",
    "#         self.index = None\n",
    "#         self.llm = None  # to be loaded later\n",
    "#         self.embed_model = None\n",
    "#         self.system_prompt = \"\"\"\n",
    "#                 You are a human being that is trying to converse with an \n",
    "#                 Alzheimer's patient. \n",
    "#                 Use the memories in the data and respond naturally.\n",
    "#                 \"\"\"\n",
    "\n",
    "#     def load_model(self):\n",
    "#         self.llm = HuggingFaceLLM(\n",
    "#                 context_window= self.context_window,\n",
    "#                 max_new_tokens= self.max_new_tokens,\n",
    "#                 generate_kwargs= self.generate_kwargs,\n",
    "#                 system_prompt= self.system_prompt,\n",
    "#                 # query_wrapper_prompt=query_wrapper_prompt,\n",
    "#                 tokenizer_name= self.tokenizer_name,\n",
    "#                 model_name= self.model_name,\n",
    "#                 device_map= self.device_map,\n",
    "#                 # uncomment this if using CUDA to reduce memory usage\n",
    "#                 model_kwargs= self.model_kwargs,\n",
    "#                 # llm_int8_enable_fp32_cpu_offload=True\n",
    "#             )\n",
    "\n",
    "#     def load_data(self, data_path = \"./data\"):\n",
    "#         # try:\n",
    "#         self.documents=SimpleDirectoryReader(\"./data\").load_data()\n",
    "#         if self.documents:\n",
    "#             print(\"Documents Loaded\")\n",
    "#         else:\n",
    "#             print(\"No Documents found, please check the path or the document format \\n The document format must be in pdf\")\n",
    "#         # except:\n",
    "#         #     print(\"Error in loading document, Simple Directory Error\")\n",
    "\n",
    "#     def prepare(self, embedding_model=\"sentence-transformers/all-mpnet-base-v2\", data_path=\"./data\"):\n",
    "#         \"\"\"Load model, data, embeddings, and prepare index.\"\"\"\n",
    "#         self.load_model()\n",
    "#         self.load_data(data_path)\n",
    "\n",
    "#         self.embed_model = LangchainEmbedding(\n",
    "#             HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "#         )\n",
    "\n",
    "#         Settings.llm = self.llm\n",
    "#         Settings.embed_model = self.embed_model\n",
    "#         Settings.chunk_size = 1024\n",
    "\n",
    "#         self.index = VectorStoreIndex.from_documents(self.documents)\n",
    "#         self.query_engine = self.index.as_query_engine()\n",
    "        \n",
    "#     def call(self, query: str):\n",
    "#         \"\"\"Perform inference using the initialized query engine.\"\"\"\n",
    "#         if self.query_engine is None:\n",
    "#             raise RuntimeError(\"Model not prepared. Call `prepare()` first.\")\n",
    "#         return self.query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8a7be3cf-134b-4708-8aac-cdf6e9ada7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_documents(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e20f32cc-fa2c-44d7-bcf0-d4cb4c8fd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, query_engine):\n",
    "        response=query_engine.query(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0e59f340-8ccc-439f-95e7-7be76f3c37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plain_Llama:\n",
    "\n",
    "    def __init__(self,\n",
    "                model = \"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "        self.model = model # meta-llama/Llama-2-7b-hf\n",
    "\n",
    "    \n",
    "    def load_plain_model_pipeline(self, device_map=\"cuda:0\"):\n",
    "        self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model, device_map = device_map, load_in_8bit=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model, use_auth_token=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "        self.llama_pipeline = pipeline(\n",
    "            \"text-generation\",  # LLM task\n",
    "            model=self.model_8bit,\n",
    "            tokenizer = self.tokenizer,\n",
    "            torch_dtype=torch.uint8,\n",
    "            device_map=\"cuda:0\",\n",
    "        )\n",
    "    \n",
    "    def get_plain_llama_response(self,prompt: str) -> None:\n",
    "        \"\"\"\n",
    "        Generate a response from the Llama model.\n",
    "    \n",
    "        Parameters:\n",
    "            prompt (str): The user's input/question for the model.\n",
    "    \n",
    "        Returns:\n",
    "            None: Prints the model's response.\n",
    "        \"\"\"\n",
    "        self.sequences = self.llama_pipeline(\n",
    "            prompt,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            max_length=256,\n",
    "        )\n",
    "    \n",
    "        return self.sequences[0]\n",
    "\n",
    "    def call_plain_model(self, prompt = \"\"):\n",
    "        self.load_plain_model_pipeline()\n",
    "        \n",
    "        self.prompt = prompt\n",
    "        self.plain_response = self.get_plain_llama_response(self.prompt)\n",
    "\n",
    "        # self.model_8bit.cpu()\n",
    "        # del self.model_8bit, checkpoint\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "        \n",
    "        return self.plain_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c7c0f-9012-4685-8d79-9968e5ab83e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
